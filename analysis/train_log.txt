Train losses:
epoch 0, global step 10, total step 721440, train lm loss: 5.229525029659271
epoch 0, global step 20, total step 721440, train lm loss: 5.834114983677864
epoch 0, global step 30, total step 721440, train lm loss: 5.86211117208004
epoch 0, global step 40, total step 721440, train lm loss: 5.766485801339149
epoch 0, global step 50, total step 721440, train lm loss: 5.645457749068737
epoch 0, global step 60, total step 721440, train lm loss: 5.329704032838345
epoch 0, global step 70, total step 721440, train lm loss: 5.009593531489372
epoch 0, global step 80, total step 721440, train lm loss: 4.364678880572319
epoch 0, global step 90, total step 721440, train lm loss: 3.884530858695507
epoch 0, global step 100, total step 721440, train lm loss: 2.8810192435979842
epoch 0, global step 110, total step 721440, train lm loss: 2.6214842766523363
epoch 0, global step 120, total step 721440, train lm loss: 2.4733968704938887
epoch 0, global step 130, total step 721440, train lm loss: 2.4007352381944655
epoch 0, global step 140, total step 721440, train lm loss: 2.3644845679402353
epoch 0, global step 150, total step 721440, train lm loss: 2.35751431286335
epoch 0, global step 160, total step 721440, train lm loss: 2.366696570813656
epoch 0, global step 170, total step 721440, train lm loss: 2.3602603957057
epoch 0, global step 180, total step 721440, train lm loss: 2.353835327923298
epoch 0, global step 190, total step 721440, train lm loss: 2.336715565621853
epoch 0, global step 200, total step 721440, train lm loss: 2.3452853918075562
epoch 0, global step 210, total step 721440, train lm loss: 2.340833891928196
epoch 0, global step 220, total step 721440, train lm loss: 2.33837680965662
epoch 0, global step 230, total step 721440, train lm loss: 2.3345323696732523
epoch 0, global step 240, total step 721440, train lm loss: 2.3314330503344536
epoch 0, global step 250, total step 721440, train lm loss: 2.3308570101857184
epoch 0, global step 260, total step 721440, train lm loss: 2.3333908766508102
epoch 0, global step 270, total step 721440, train lm loss: 2.325341211259365
epoch 0, global step 280, total step 721440, train lm loss: 2.337738110125065
epoch 0, global step 290, total step 721440, train lm loss: 2.3268908604979517
epoch 0, global step 300, total step 721440, train lm loss: 2.327390603721142
epoch 0, global step 310, total step 721440, train lm loss: 2.331318974494934
epoch 0, global step 320, total step 721440, train lm loss: 2.3219439908862114
epoch 0, global step 330, total step 721440, train lm loss: 2.3276249289512636
epoch 0, global step 340, total step 721440, train lm loss: 2.311515174806118
epoch 0, global step 350, total step 721440, train lm loss: 2.3235047325491904
epoch 0, global step 360, total step 721440, train lm loss: 2.3285728216171266
epoch 0, global step 370, total step 721440, train lm loss: 2.320814919471741
epoch 0, global step 380, total step 721440, train lm loss: 2.3216102585196494
epoch 0, global step 390, total step 721440, train lm loss: 2.318808250129223
epoch 0, global step 400, total step 721440, train lm loss: 2.3151075705885886
epoch 0, global step 410, total step 721440, train lm loss: 2.3173935383558275
epoch 0, global step 420, total step 721440, train lm loss: 2.314344935119152
epoch 0, global step 430, total step 721440, train lm loss: 2.3184902906417846
epoch 0, global step 440, total step 721440, train lm loss: 2.3182215332984923
epoch 0, global step 450, total step 721440, train lm loss: 2.314252142608166
epoch 0, global step 460, total step 721440, train lm loss: 2.3233537912368774
epoch 0, global step 470, total step 721440, train lm loss: 2.3078522801399233
epoch 0, global step 480, total step 721440, train lm loss: 2.315144295990467
epoch 0, global step 490, total step 721440, train lm loss: 2.3185377180576325
epoch 0, global step 500, total step 721440, train lm loss: 2.318907618522644
epoch 0, global step 510, total step 721440, train lm loss: 2.3187376841902734
epoch 0, global step 520, total step 721440, train lm loss: 2.3198358178138734
epoch 0, global step 530, total step 721440, train lm loss: 2.317951647937298
epoch 0, global step 540, total step 721440, train lm loss: 2.3192644640803337
epoch 0, global step 550, total step 721440, train lm loss: 2.313668654859066
epoch 0, global step 560, total step 721440, train lm loss: 2.3174159228801727
epoch 0, global step 570, total step 721440, train lm loss: 2.3236449524760245
epoch 0, global step 580, total step 721440, train lm loss: 2.315330931544304
epoch 0, global step 590, total step 721440, train lm loss: 2.3116281792521476
epoch 0, global step 600, total step 721440, train lm loss: 2.3074802294373513
epoch 0, global step 610, total step 721440, train lm loss: 2.316308981180191
epoch 0, global step 620, total step 721440, train lm loss: 2.3177919924259185
epoch 0, global step 630, total step 721440, train lm loss: 2.3149642303586004
epoch 0, global step 640, total step 721440, train lm loss: 2.3104051291942596
epoch 0, global step 650, total step 721440, train lm loss: 2.317662827670574
epoch 0, global step 660, total step 721440, train lm loss: 2.3167750746011735
epoch 0, global step 670, total step 721440, train lm loss: 2.3193961918354034
epoch 0, global step 680, total step 721440, train lm loss: 2.312516060471535
epoch 0, global step 690, total step 721440, train lm loss: 2.3139325797557833
epoch 0, global step 700, total step 721440, train lm loss: 2.3122460782527923
epoch 0, global step 710, total step 721440, train lm loss: 2.319663752615452
epoch 0, global step 720, total step 721440, train lm loss: 2.30771127641201
epoch 0, global step 730, total step 721440, train lm loss: 2.3194175943732263
epoch 0, global step 740, total step 721440, train lm loss: 2.3202121794223785
epoch 0, global step 750, total step 721440, train lm loss: 2.316748560965061
epoch 0, global step 760, total step 721440, train lm loss: 2.318739002943039
epoch 0, global step 770, total step 721440, train lm loss: 2.3175625830888746
epoch 0, global step 780, total step 721440, train lm loss: 2.312310867011547
epoch 0, global step 790, total step 721440, train lm loss: 2.314827415347099
epoch 0, global step 800, total step 721440, train lm loss: 2.313301350176334
epoch 0, global step 810, total step 721440, train lm loss: 2.3090871661901473
epoch 0, global step 820, total step 721440, train lm loss: 2.313825298845768
epoch 0, global step 830, total step 721440, train lm loss: 2.3106497496366503
epoch 0, global step 840, total step 721440, train lm loss: 2.3183642491698264
epoch 0, global step 850, total step 721440, train lm loss: 2.3085081607103346
epoch 0, global step 860, total step 721440, train lm loss: 2.3041660621762277
epoch 0, global step 870, total step 721440, train lm loss: 2.313885119557381
epoch 0, global step 880, total step 721440, train lm loss: 2.3110083445906637
epoch 0, global step 890, total step 721440, train lm loss: 2.312020666897297
epoch 0, global step 900, total step 721440, train lm loss: 2.309245267510414
epoch 0, global step 910, total step 721440, train lm loss: 2.3171613991260527
epoch 0, global step 920, total step 721440, train lm loss: 2.3142043143510818
epoch 0, global step 930, total step 721440, train lm loss: 2.3114761859178543
epoch 0, global step 940, total step 721440, train lm loss: 2.313426171243191
epoch 0, global step 950, total step 721440, train lm loss: 2.31530881524086
epoch 0, global step 960, total step 721440, train lm loss: 2.319558474421501
epoch 0, global step 970, total step 721440, train lm loss: 2.307578819990158
epoch 0, global step 980, total step 721440, train lm loss: 2.3177847623825074
epoch 0, global step 990, total step 721440, train lm loss: 2.3089370489120484
epoch 0, global step 1000, total step 721440, train lm loss: 2.314281974732876
epoch 0, global step 1010, total step 721440, train lm loss: 2.3182952225208284
epoch 0, global step 1020, total step 721440, train lm loss: 2.311796259880066
epoch 0, global step 1030, total step 721440, train lm loss: 2.314209893345833
epoch 0, global step 1040, total step 721440, train lm loss: 2.3094204872846604
epoch 0, global step 1050, total step 721440, train lm loss: 2.3125385239720346
epoch 0, global step 1060, total step 721440, train lm loss: 2.310771073400974
epoch 0, global step 1070, total step 721440, train lm loss: 2.3033306121826174
epoch 0, global step 1080, total step 721440, train lm loss: 2.3126995146274565
epoch 0, global step 1090, total step 721440, train lm loss: 2.320576713979244
epoch 0, global step 1100, total step 721440, train lm loss: 2.3178908795118334
epoch 0, global step 1110, total step 721440, train lm loss: 2.3097704097628595
epoch 0, global step 1120, total step 721440, train lm loss: 2.313782000541687
epoch 0, global step 1130, total step 721440, train lm loss: 2.3080529853701592
epoch 0, global step 1140, total step 721440, train lm loss: 2.310682325065136
epoch 0, global step 1150, total step 721440, train lm loss: 2.313951091468334
epoch 0, global step 1160, total step 721440, train lm loss: 2.312278838455677
epoch 0, global step 1170, total step 721440, train lm loss: 2.311537930369377
epoch 0, global step 1180, total step 721440, train lm loss: 2.3175192207098005
epoch 0, global step 1190, total step 721440, train lm loss: 2.3087804570794104
epoch 0, global step 1200, total step 721440, train lm loss: 2.312305209040642
epoch 0, global step 1210, total step 721440, train lm loss: 2.3124971225857736
epoch 0, global step 1220, total step 721440, train lm loss: 2.308689771592617
epoch 0, global step 1230, total step 721440, train lm loss: 2.312158203125
epoch 0, global step 1240, total step 721440, train lm loss: 2.3113619282841684
epoch 0, global step 1250, total step 721440, train lm loss: 2.313557727634907
epoch 0, global step 1260, total step 721440, train lm loss: 2.3171725049614906
epoch 0, global step 1270, total step 721440, train lm loss: 2.301659220457077
epoch 0, global step 1280, total step 721440, train lm loss: 2.3170758694410325
epoch 0, global step 1290, total step 721440, train lm loss: 2.304769343137741
epoch 0, global step 1300, total step 721440, train lm loss: 2.305746528506279
epoch 0, global step 1310, total step 721440, train lm loss: 2.303060512244701
epoch 0, global step 1320, total step 721440, train lm loss: 2.304008661210537
epoch 0, global step 1330, total step 721440, train lm loss: 2.3045833364129065
epoch 0, global step 1340, total step 721440, train lm loss: 2.307708641886711
epoch 0, global step 1350, total step 721440, train lm loss: 2.30964031368494
epoch 0, global step 1360, total step 721440, train lm loss: 2.303308889269829
epoch 0, global step 1370, total step 721440, train lm loss: 2.2894920721650123
epoch 0, global step 1380, total step 721440, train lm loss: 2.3025319457054136
epoch 0, global step 1390, total step 721440, train lm loss: 2.3020091354846954
epoch 0, global step 1400, total step 721440, train lm loss: 2.2987759858369827
epoch 0, global step 1410, total step 721440, train lm loss: 2.3101697459816934
epoch 0, global step 1420, total step 721440, train lm loss: 2.305538594722748
epoch 0, global step 1430, total step 721440, train lm loss: 2.298804667592049
epoch 0, global step 1440, total step 721440, train lm loss: 2.2959348902106287
epoch 0, global step 1450, total step 721440, train lm loss: 2.300491726398468
epoch 0, global step 1460, total step 721440, train lm loss: 2.303975048661232
epoch 0, global step 1470, total step 721440, train lm loss: 2.305366279184818
epoch 0, global step 1480, total step 721440, train lm loss: 2.3055105686187742
epoch 0, global step 1490, total step 721440, train lm loss: 2.2951816260814666
epoch 0, global step 1500, total step 721440, train lm loss: 2.3105197668075563
epoch 0, global step 1510, total step 721440, train lm loss: 2.288314163684845
epoch 0, global step 1520, total step 721440, train lm loss: 2.301810450106859
epoch 0, global step 1530, total step 721440, train lm loss: 2.270653218775988
epoch 0, global step 1540, total step 721440, train lm loss: 2.3137139946222307
epoch 0, global step 1550, total step 721440, train lm loss: 2.2826117247343065
epoch 0, global step 1560, total step 721440, train lm loss: 2.2983342900872232
epoch 0, global step 1570, total step 721440, train lm loss: 2.2923842072486877
epoch 0, global step 1580, total step 721440, train lm loss: 2.281312258541584
epoch 0, global step 1590, total step 721440, train lm loss: 2.2868168354034424
epoch 0, global step 1600, total step 721440, train lm loss: 2.301605997979641
epoch 0, global step 1610, total step 721440, train lm loss: 2.291751766204834
epoch 0, global step 1620, total step 721440, train lm loss: 2.2963404804468155
epoch 0, global step 1630, total step 721440, train lm loss: 2.3096737384796144
epoch 0, global step 1640, total step 721440, train lm loss: 2.2892676785588266
epoch 0, global step 1650, total step 721440, train lm loss: 2.297708459198475
epoch 0, global step 1660, total step 721440, train lm loss: 2.2818003937602045
epoch 0, global step 1670, total step 721440, train lm loss: 2.282762776315212
epoch 0, global step 1680, total step 721440, train lm loss: 2.2902656383812428
epoch 0, global step 1690, total step 721440, train lm loss: 2.2915496274828913
epoch 0, global step 1700, total step 721440, train lm loss: 2.285445588082075
epoch 0, global step 1710, total step 721440, train lm loss: 2.288371941447258
epoch 0, global step 1720, total step 721440, train lm loss: 2.27260203063488
epoch 0, global step 1730, total step 721440, train lm loss: 2.2822593793272974
epoch 0, global step 1740, total step 721440, train lm loss: 2.2902299530804155
epoch 0, global step 1750, total step 721440, train lm loss: 2.2938878506422045
epoch 0, global step 1760, total step 721440, train lm loss: 2.297809061408043
epoch 0, global step 1770, total step 721440, train lm loss: 2.2712972164154053
epoch 0, global step 1780, total step 721440, train lm loss: 2.2764302633702753
epoch 0, global step 1790, total step 721440, train lm loss: 2.2725727051496505
epoch 0, global step 1800, total step 721440, train lm loss: 2.282628731429577
epoch 0, global step 1810, total step 721440, train lm loss: 2.2734250843524935
epoch 0, global step 1820, total step 721440, train lm loss: 2.2783057168126106
epoch 0, global step 1830, total step 721440, train lm loss: 2.2741802386939525
epoch 0, global step 1840, total step 721440, train lm loss: 2.2732239037752153
epoch 0, global step 1850, total step 721440, train lm loss: 2.2617458276450635
epoch 0, global step 1860, total step 721440, train lm loss: 2.253297804296017
epoch 0, global step 1870, total step 721440, train lm loss: 2.2498232036828996
epoch 0, global step 1880, total step 721440, train lm loss: 2.272449687868357
epoch 0, global step 1890, total step 721440, train lm loss: 2.2591499999165534
epoch 0, global step 1900, total step 721440, train lm loss: 2.2614624433219435
epoch 0, global step 1910, total step 721440, train lm loss: 2.242942604422569
epoch 0, global step 1920, total step 721440, train lm loss: 2.242007339000702
epoch 0, global step 1930, total step 721440, train lm loss: 2.2214497990906237
epoch 0, global step 1940, total step 721440, train lm loss: 2.213021322339773
epoch 0, global step 1950, total step 721440, train lm loss: 2.180708350241184
epoch 0, global step 1960, total step 721440, train lm loss: 2.1725378923118113
epoch 0, global step 1970, total step 721440, train lm loss: 2.1565396897494793
epoch 0, global step 1980, total step 721440, train lm loss: 2.154407881945372
epoch 0, global step 1990, total step 721440, train lm loss: 2.115602749586105
epoch 0, global step 2000, total step 721440, train lm loss: 2.0913347780704497
epoch 0, global step 2010, total step 721440, train lm loss: 2.0780581668019296
epoch 0, global step 2020, total step 721440, train lm loss: 2.0815492533147335
epoch 0, global step 2030, total step 721440, train lm loss: 2.0712127573788166
epoch 0, global step 2040, total step 721440, train lm loss: 2.0622091814875603
epoch 0, global step 2050, total step 721440, train lm loss: 2.0258288525044916
epoch 0, global step 2060, total step 721440, train lm loss: 2.014015879482031
epoch 0, global step 2070, total step 721440, train lm loss: 1.995893481373787
epoch 0, global step 2080, total step 721440, train lm loss: 1.9367637276649474
epoch 0, global step 2090, total step 721440, train lm loss: 1.915386402606964
epoch 0, global step 2100, total step 721440, train lm loss: 1.945624504983425
epoch 0, global step 2110, total step 721440, train lm loss: 1.917601866275072
epoch 0, global step 2120, total step 721440, train lm loss: 1.87339161708951
epoch 0, global step 2130, total step 721440, train lm loss: 1.9114607907831669
epoch 0, global step 2140, total step 721440, train lm loss: 1.8870493002235889
epoch 0, global step 2150, total step 721440, train lm loss: 1.7894405372440816
epoch 0, global step 2160, total step 721440, train lm loss: 1.8091608449816703
epoch 0, global step 2170, total step 721440, train lm loss: 1.8190140906721353
epoch 0, global step 2180, total step 721440, train lm loss: 1.7971904933452607
epoch 0, global step 2190, total step 721440, train lm loss: 1.7515063554048538
epoch 0, global step 2200, total step 721440, train lm loss: 1.6991001784801483
epoch 0, global step 2210, total step 721440, train lm loss: 1.714870745688677
epoch 0, global step 2220, total step 721440, train lm loss: 1.6975707679986953
epoch 0, global step 2230, total step 721440, train lm loss: 1.7102446228265762
epoch 0, global step 2240, total step 721440, train lm loss: 1.724106765910983
epoch 0, global step 2250, total step 721440, train lm loss: 1.7429916042834521
epoch 0, global step 2260, total step 721440, train lm loss: 1.644964999333024
epoch 0, global step 2270, total step 721440, train lm loss: 1.6670727219432593
epoch 0, global step 2280, total step 721440, train lm loss: 1.679181545227766
epoch 0, global step 2290, total step 721440, train lm loss: 1.598752562701702
epoch 0, global step 2300, total step 721440, train lm loss: 1.6432330273091793
epoch 0, global step 2310, total step 721440, train lm loss: 1.5963186752051115
epoch 0, global step 2320, total step 721440, train lm loss: 1.6576350949704648
epoch 0, global step 2330, total step 721440, train lm loss: 1.569722306728363
epoch 0, global step 2340, total step 721440, train lm loss: 1.588700082153082
epoch 0, global step 2350, total step 721440, train lm loss: 1.5582356449216603
epoch 0, global step 2360, total step 721440, train lm loss: 1.5811918072402478
epoch 0, global step 2370, total step 721440, train lm loss: 1.5821924459189176
epoch 0, global step 2380, total step 721440, train lm loss: 1.5078493002802134
epoch 0, global step 2390, total step 721440, train lm loss: 1.535304006934166
epoch 0, global step 2400, total step 721440, train lm loss: 1.5214368775486946
epoch 0, global step 2410, total step 721440, train lm loss: 1.4805388424545527
epoch 0, global step 2420, total step 721440, train lm loss: 1.523216700926423
epoch 0, global step 2430, total step 721440, train lm loss: 1.485093367472291
epoch 0, global step 2440, total step 721440, train lm loss: 1.4874458249658347
epoch 0, global step 2450, total step 721440, train lm loss: 1.4277512729167938
epoch 0, global step 2460, total step 721440, train lm loss: 1.4024838589131832
epoch 0, global step 2470, total step 721440, train lm loss: 1.4729501463472843
epoch 0, global step 2480, total step 721440, train lm loss: 1.401428972184658
epoch 0, global step 2490, total step 721440, train lm loss: 1.4708967234939336
epoch 0, global step 2500, total step 721440, train lm loss: 1.3983444500714541
epoch 0, global step 2510, total step 721440, train lm loss: 1.4407628823071719
epoch 0, global step 2520, total step 721440, train lm loss: 1.4050269633531571
epoch 0, global step 2530, total step 721440, train lm loss: 1.44566754065454
epoch 0, global step 2540, total step 721440, train lm loss: 1.4152040500193834
epoch 0, global step 2550, total step 721440, train lm loss: 1.3828588161617517
epoch 0, global step 2560, total step 721440, train lm loss: 1.3745338674634695
epoch 0, global step 2570, total step 721440, train lm loss: 1.3779295604676007
epoch 0, global step 2580, total step 721440, train lm loss: 1.39790323600173
epoch 0, global step 2590, total step 721440, train lm loss: 1.3737985860556363
epoch 0, global step 2600, total step 721440, train lm loss: 1.3572984036058187
epoch 0, global step 2610, total step 721440, train lm loss: 1.385736957192421
epoch 0, global step 2620, total step 721440, train lm loss: 1.3867632679641246
epoch 0, global step 2630, total step 721440, train lm loss: 1.3489365339279176
epoch 0, global step 2640, total step 721440, train lm loss: 1.3205795492976904
epoch 0, global step 2650, total step 721440, train lm loss: 1.3222026333212853
epoch 0, global step 2660, total step 721440, train lm loss: 1.3303814057260752
epoch 0, global step 2670, total step 721440, train lm loss: 1.3636318422853946
epoch 0, global step 2680, total step 721440, train lm loss: 1.2979366894811393
epoch 0, global step 2690, total step 721440, train lm loss: 1.2578630112111568
epoch 0, global step 2700, total step 721440, train lm loss: 1.247414581477642
epoch 0, global step 2710, total step 721440, train lm loss: 1.3292827434837817
epoch 0, global step 2720, total step 721440, train lm loss: 1.2518859423696995
epoch 0, global step 2730, total step 721440, train lm loss: 1.3049210406839848
epoch 0, global step 2740, total step 721440, train lm loss: 1.300572818890214
epoch 0, global step 2750, total step 721440, train lm loss: 1.290186619386077
epoch 0, global step 2760, total step 721440, train lm loss: 1.215787560492754
epoch 0, global step 2770, total step 721440, train lm loss: 1.2933425068855287
epoch 0, global step 2780, total step 721440, train lm loss: 1.3068508269265293
epoch 0, global step 2790, total step 721440, train lm loss: 1.274482313171029
epoch 0, global step 2800, total step 721440, train lm loss: 1.241212942264974
epoch 0, global step 2810, total step 721440, train lm loss: 1.223750190064311
epoch 0, global step 2820, total step 721440, train lm loss: 1.2641271129250526
epoch 0, global step 2830, total step 721440, train lm loss: 1.2461716197431087
epoch 0, global step 2840, total step 721440, train lm loss: 1.1842261351644994
epoch 0, global step 2850, total step 721440, train lm loss: 1.212585829757154
epoch 0, global step 2860, total step 721440, train lm loss: 1.196424564346671
epoch 0, global step 2870, total step 721440, train lm loss: 1.1897581841796636
epoch 0, global step 2880, total step 721440, train lm loss: 1.2135015442967414
epoch 0, global step 2890, total step 721440, train lm loss: 1.2028836525976658
epoch 0, global step 2900, total step 721440, train lm loss: 1.2325823517516254
epoch 0, global step 2910, total step 721440, train lm loss: 1.1518273985013365
epoch 0, global step 2920, total step 721440, train lm loss: 1.1957160033285619
epoch 0, global step 2930, total step 721440, train lm loss: 1.1399976070970297
epoch 0, global step 2940, total step 721440, train lm loss: 1.216773521527648
epoch 0, global step 2950, total step 721440, train lm loss: 1.202297443524003
epoch 0, global step 2960, total step 721440, train lm loss: 1.2389083782210946
epoch 0, global step 2970, total step 721440, train lm loss: 1.22531150970608
epoch 0, global step 2980, total step 721440, train lm loss: 1.2124069813638925
epoch 0, global step 2990, total step 721440, train lm loss: 1.1824617985635997
epoch 0, global step 3000, total step 721440, train lm loss: 1.2309800568968057
epoch 0, global step 3010, total step 721440, train lm loss: 1.1474949695169925
epoch 0, global step 3020, total step 721440, train lm loss: 1.1368187369778753
epoch 0, global step 3030, total step 721440, train lm loss: 1.1823132760822772
epoch 0, global step 3040, total step 721440, train lm loss: 1.1258276492357253
epoch 0, global step 3050, total step 721440, train lm loss: 1.20870977435261
epoch 0, global step 3060, total step 721440, train lm loss: 1.1884287295863032
epoch 0, global step 3070, total step 721440, train lm loss: 1.1782364735379816
epoch 0, global step 3080, total step 721440, train lm loss: 1.1388296162709595
epoch 0, global step 3090, total step 721440, train lm loss: 1.137814000248909
epoch 0, global step 3100, total step 721440, train lm loss: 1.1715928874909878
epoch 0, global step 3110, total step 721440, train lm loss: 1.2026347562670707
epoch 0, global step 3120, total step 721440, train lm loss: 1.1211478481069208
epoch 0, global step 3130, total step 721440, train lm loss: 1.1035187158733606
epoch 0, global step 3140, total step 721440, train lm loss: 1.1315096581354738
epoch 0, global step 3150, total step 721440, train lm loss: 1.144028495810926
epoch 0, global step 3160, total step 721440, train lm loss: 1.160300700739026
epoch 0, global step 3170, total step 721440, train lm loss: 1.1442155078053475
epoch 0, global step 3180, total step 721440, train lm loss: 1.181522412225604
epoch 0, global step 3190, total step 721440, train lm loss: 1.146349915675819
epoch 0, global step 3200, total step 721440, train lm loss: 1.183961664326489
epoch 0, global step 3210, total step 721440, train lm loss: 1.086314426921308
epoch 0, global step 3220, total step 721440, train lm loss: 1.1347507420927285
epoch 0, global step 3230, total step 721440, train lm loss: 1.1226333282887935
epoch 0, global step 3240, total step 721440, train lm loss: 1.124824920296669
epoch 0, global step 3250, total step 721440, train lm loss: 1.1105810791254043
epoch 0, global step 3260, total step 721440, train lm loss: 1.077809285931289
epoch 0, global step 3270, total step 721440, train lm loss: 1.1065539073199033
epoch 0, global step 3280, total step 721440, train lm loss: 1.075124983675778
epoch 0, global step 3290, total step 721440, train lm loss: 1.0876374539919198
epoch 0, global step 3300, total step 721440, train lm loss: 1.095992462337017
epoch 0, global step 3310, total step 721440, train lm loss: 1.0334545254707337
epoch 0, global step 3320, total step 721440, train lm loss: 1.026072498690337
epoch 0, global step 3330, total step 721440, train lm loss: 1.1852764334529637
epoch 0, global step 3340, total step 721440, train lm loss: 1.09381879940629
epoch 0, global step 3350, total step 721440, train lm loss: 1.1097767008468509
epoch 0, global step 3360, total step 721440, train lm loss: 1.0644212992861868
epoch 0, global step 3370, total step 721440, train lm loss: 1.1109660155139864
epoch 0, global step 3380, total step 721440, train lm loss: 1.0837645949795842
epoch 0, global step 3390, total step 721440, train lm loss: 1.087502584606409
epoch 0, global step 3400, total step 721440, train lm loss: 1.0768171133473516
epoch 0, global step 3410, total step 721440, train lm loss: 1.052109766472131
epoch 0, global step 3420, total step 721440, train lm loss: 1.0395909769460558
epoch 0, global step 3430, total step 721440, train lm loss: 1.046647092886269
epoch 0, global step 3440, total step 721440, train lm loss: 1.0500034177675843
epoch 0, global step 3450, total step 721440, train lm loss: 1.0546588132157921
epoch 0, global step 3460, total step 721440, train lm loss: 1.1084230260923504
epoch 0, global step 3470, total step 721440, train lm loss: 1.0418097296729685
epoch 0, global step 3480, total step 721440, train lm loss: 1.0195646205917002
epoch 0, global step 3490, total step 721440, train lm loss: 1.0254626291804017
epoch 0, global step 3500, total step 721440, train lm loss: 1.0663847291842103
epoch 0, global step 3510, total step 721440, train lm loss: 1.0801495738327502
epoch 0, global step 3520, total step 721440, train lm loss: 1.0647512255236506
epoch 0, global step 3530, total step 721440, train lm loss: 1.048780190013349
epoch 0, global step 3540, total step 721440, train lm loss: 0.9874482670798898
epoch 0, global step 3550, total step 721440, train lm loss: 1.0530199952423573
epoch 0, global step 3560, total step 721440, train lm loss: 1.0529161585494875
epoch 0, global step 3570, total step 721440, train lm loss: 1.0769051551818847
epoch 0, global step 3580, total step 721440, train lm loss: 1.0864767119288445
epoch 0, global step 3590, total step 721440, train lm loss: 1.0253984233364464
epoch 0, global step 3600, total step 721440, train lm loss: 1.0245600508525967
epoch 0, global step 3610, total step 721440, train lm loss: 0.9942993502132594
epoch 0, global step 3620, total step 721440, train lm loss: 1.0112331008538604
epoch 0, global step 3630, total step 721440, train lm loss: 1.0675009252503513
epoch 0, global step 3640, total step 721440, train lm loss: 1.0673773072659969
epoch 0, global step 3650, total step 721440, train lm loss: 1.0466923708096147
epoch 0, global step 3660, total step 721440, train lm loss: 1.0392792591825128
epoch 0, global step 3670, total step 721440, train lm loss: 0.9465435419231654
epoch 0, global step 3680, total step 721440, train lm loss: 0.9461838267743587
epoch 0, global step 3690, total step 721440, train lm loss: 1.0196295715868473
epoch 0, global step 3700, total step 721440, train lm loss: 1.0041637999936939
epoch 0, global step 3710, total step 721440, train lm loss: 0.99168107137084
epoch 0, global step 3720, total step 721440, train lm loss: 1.0160440299659967
epoch 0, global step 3730, total step 721440, train lm loss: 1.0739104411564768
epoch 0, global step 3740, total step 721440, train lm loss: 0.9484820913523435
epoch 0, global step 3750, total step 721440, train lm loss: 1.0052673912607133
epoch 0, global step 3760, total step 721440, train lm loss: 0.9935073956847191
epoch 0, global step 3770, total step 721440, train lm loss: 1.0622116455808281
epoch 0, global step 3780, total step 721440, train lm loss: 0.974344346486032
epoch 0, global step 3790, total step 721440, train lm loss: 0.9863307777792215
epoch 0, global step 3800, total step 721440, train lm loss: 0.9889202507212758
epoch 0, global step 3810, total step 721440, train lm loss: 0.9381518309935928
epoch 0, global step 3820, total step 721440, train lm loss: 1.0018980473279953
epoch 0, global step 3830, total step 721440, train lm loss: 0.9818112341687083
epoch 0, global step 3840, total step 721440, train lm loss: 0.9834448921494185
epoch 0, global step 3850, total step 721440, train lm loss: 0.9776633689180017
epoch 0, global step 3860, total step 721440, train lm loss: 1.0050822359509766
epoch 0, global step 3870, total step 721440, train lm loss: 0.9719665052369237
epoch 0, global step 3880, total step 721440, train lm loss: 0.9668231820687652
epoch 0, global step 3890, total step 721440, train lm loss: 1.0214490249752999
epoch 0, global step 3900, total step 721440, train lm loss: 1.0406424069777132
epoch 0, global step 3910, total step 721440, train lm loss: 0.9099378537386655
epoch 0, global step 3920, total step 721440, train lm loss: 1.0139603361487388
epoch 0, global step 3930, total step 721440, train lm loss: 0.9455951329320669
epoch 0, global step 3940, total step 721440, train lm loss: 0.9830451838672161
epoch 0, global step 3950, total step 721440, train lm loss: 1.0040838986635208
epoch 0, global step 3960, total step 721440, train lm loss: 0.9556079297326505
epoch 0, global step 3970, total step 721440, train lm loss: 1.0092418422922491
epoch 0, global step 3980, total step 721440, train lm loss: 0.9211643157526851
epoch 0, global step 3990, total step 721440, train lm loss: 0.9353231483139097
epoch 0, global step 4000, total step 721440, train lm loss: 0.9498103713616729
epoch 0, global step 4010, total step 721440, train lm loss: 0.9997292338870466
epoch 0, global step 4020, total step 721440, train lm loss: 0.952072262391448
epoch 0, global step 4030, total step 721440, train lm loss: 0.9689776936545968
epoch 0, global step 4040, total step 721440, train lm loss: 0.9617801943793893
epoch 0, global step 4050, total step 721440, train lm loss: 0.9758615954779088
epoch 0, global step 4060, total step 721440, train lm loss: 0.916724632307887
epoch 0, global step 4070, total step 721440, train lm loss: 0.933338763564825
epoch 0, global step 4080, total step 721440, train lm loss: 0.9845342062413692
epoch 0, global step 4090, total step 721440, train lm loss: 0.9541376768611372
epoch 0, global step 4100, total step 721440, train lm loss: 0.9242305027320981
epoch 0, global step 4110, total step 721440, train lm loss: 0.9219451964832842
epoch 0, global step 4120, total step 721440, train lm loss: 0.9518901037983596
epoch 0, global step 4130, total step 721440, train lm loss: 0.9868708778172731
epoch 0, global step 4140, total step 721440, train lm loss: 0.9445632171817124
epoch 0, global step 4150, total step 721440, train lm loss: 0.9496066616848111
epoch 0, global step 4160, total step 721440, train lm loss: 0.9750358274206519
epoch 0, global step 4170, total step 721440, train lm loss: 0.9479690164327621
epoch 0, global step 4180, total step 721440, train lm loss: 0.9514753686264157
epoch 0, global step 4190, total step 721440, train lm loss: 0.9294899992644787
epoch 0, global step 4200, total step 721440, train lm loss: 0.9971977992914617
epoch 0, global step 4210, total step 721440, train lm loss: 0.9250763514079153
epoch 0, global step 4220, total step 721440, train lm loss: 0.9204429375007749
epoch 0, global step 4230, total step 721440, train lm loss: 0.8734884412027896
epoch 0, global step 4240, total step 721440, train lm loss: 0.9189438298344612
epoch 0, global step 4250, total step 721440, train lm loss: 0.9320727387443185
epoch 0, global step 4260, total step 721440, train lm loss: 0.9976508961990476
epoch 0, global step 4270, total step 721440, train lm loss: 0.9223069904372097
epoch 0, global step 4280, total step 721440, train lm loss: 0.8554365027695894
epoch 0, global step 4290, total step 721440, train lm loss: 0.9510945688933135
epoch 0, global step 4300, total step 721440, train lm loss: 0.9649200480431318
epoch 0, global step 4310, total step 721440, train lm loss: 0.9446212605573236
epoch 0, global step 4320, total step 721440, train lm loss: 0.9248727846890687
epoch 0, global step 4330, total step 721440, train lm loss: 0.8612527600489557
epoch 0, global step 4340, total step 721440, train lm loss: 0.9038360207341611
epoch 0, global step 4350, total step 721440, train lm loss: 0.9918917577713728
epoch 0, global step 4360, total step 721440, train lm loss: 0.9461774834431708
epoch 0, global step 4370, total step 721440, train lm loss: 0.9702816006727517
epoch 0, global step 4380, total step 721440, train lm loss: 0.905863456428051
epoch 0, global step 4390, total step 721440, train lm loss: 0.9439546863548458
epoch 0, global step 4400, total step 721440, train lm loss: 0.892158966511488
epoch 0, global step 4410, total step 721440, train lm loss: 0.9310631344094873
epoch 0, global step 4420, total step 721440, train lm loss: 0.9039957668632269
epoch 0, global step 4430, total step 721440, train lm loss: 0.9070508077740669
epoch 0, global step 4440, total step 721440, train lm loss: 0.8886757840402424
epoch 0, global step 4450, total step 721440, train lm loss: 0.924852754548192
epoch 0, global step 4460, total step 721440, train lm loss: 0.9408272091299296
epoch 0, global step 4470, total step 721440, train lm loss: 0.9328278373926878
epoch 0, global step 4480, total step 721440, train lm loss: 0.8738688104785979
epoch 0, global step 4490, total step 721440, train lm loss: 0.8951132974587381
epoch 0, global step 4500, total step 721440, train lm loss: 0.8216599230654538
epoch 1, global step 4510, total step 721440, train lm loss: 0.9601905243471265
epoch 1, global step 4520, total step 721440, train lm loss: 0.9240733199752867
epoch 1, global step 4530, total step 721440, train lm loss: 0.948831850849092
epoch 1, global step 4540, total step 721440, train lm loss: 0.9538617849349975
epoch 1, global step 4550, total step 721440, train lm loss: 0.8807402118109167
epoch 1, global step 4560, total step 721440, train lm loss: 0.941079755499959
epoch 1, global step 4570, total step 721440, train lm loss: 0.918859675899148
epoch 1, global step 4580, total step 721440, train lm loss: 0.8723817627876997
epoch 1, global step 4590, total step 721440, train lm loss: 0.9538912558928132
epoch 1, global step 4600, total step 721440, train lm loss: 0.8710105242207646
epoch 1, global step 4610, total step 721440, train lm loss: 0.914957824209705
epoch 1, global step 4620, total step 721440, train lm loss: 0.9283001450821757
epoch 1, global step 4630, total step 721440, train lm loss: 0.95064164288342
epoch 1, global step 4640, total step 721440, train lm loss: 0.8440298396162689
epoch 1, global step 4650, total step 721440, train lm loss: 0.9260816905647516
epoch 1, global step 4660, total step 721440, train lm loss: 0.8875541758257895
epoch 1, global step 4670, total step 721440, train lm loss: 0.8893888001330197
epoch 1, global step 4680, total step 721440, train lm loss: 0.8563662313390523
epoch 1, global step 4690, total step 721440, train lm loss: 0.8629556766711175
epoch 1, global step 4700, total step 721440, train lm loss: 0.8277486846782267
epoch 1, global step 4710, total step 721440, train lm loss: 0.870249037630856
epoch 1, global step 4720, total step 721440, train lm loss: 0.8747521502897143
epoch 1, global step 4730, total step 721440, train lm loss: 0.8710419253446162
epoch 1, global step 4740, total step 721440, train lm loss: 0.8548530159052461
epoch 1, global step 4750, total step 721440, train lm loss: 0.8867536840960384
epoch 1, global step 4760, total step 721440, train lm loss: 0.9170529283583164
epoch 1, global step 4770, total step 721440, train lm loss: 0.8235606018453836
epoch 1, global step 4780, total step 721440, train lm loss: 0.9212476715445519
epoch 1, global step 4790, total step 721440, train lm loss: 0.8409998485818505
epoch 1, global step 4800, total step 721440, train lm loss: 0.9082234516739845
epoch 1, global step 4810, total step 721440, train lm loss: 0.8682254672981798
epoch 1, global step 4820, total step 721440, train lm loss: 0.9077942204661668
epoch 1, global step 4830, total step 721440, train lm loss: 0.8461301770992578
epoch 1, global step 4840, total step 721440, train lm loss: 0.8935580543242395
epoch 1, global step 4850, total step 721440, train lm loss: 0.8688272872008383
epoch 1, global step 4860, total step 721440, train lm loss: 0.8717985868453979
epoch 1, global step 4870, total step 721440, train lm loss: 0.8478962399065495
epoch 1, global step 4880, total step 721440, train lm loss: 0.8916341047734022
epoch 1, global step 4890, total step 721440, train lm loss: 0.8613817419856786
epoch 1, global step 4900, total step 721440, train lm loss: 0.8585275179706514
epoch 1, global step 4910, total step 721440, train lm loss: 0.8503876030445099
epoch 1, global step 4920, total step 721440, train lm loss: 0.8737498773261905
epoch 1, global step 4930, total step 721440, train lm loss: 0.8120379207190126
epoch 1, global step 4940, total step 721440, train lm loss: 0.8591479834169149
epoch 1, global step 4950, total step 721440, train lm loss: 0.8441632447764278
epoch 1, global step 4960, total step 721440, train lm loss: 0.8230685075744987
epoch 1, global step 4970, total step 721440, train lm loss: 0.8726962296292186
epoch 1, global step 4980, total step 721440, train lm loss: 0.8435842376202345
epoch 1, global step 4990, total step 721440, train lm loss: 0.8277097241021693
epoch 1, global step 5000, total step 721440, train lm loss: 0.8880971849896013
epoch 1, global step 5010, total step 721440, train lm loss: 0.866316968947649
epoch 1, global step 5020, total step 721440, train lm loss: 0.8605227772146463
epoch 1, global step 5030, total step 721440, train lm loss: 0.8627770495600998
epoch 1, global step 5040, total step 721440, train lm loss: 0.8379945819731802
epoch 1, global step 5050, total step 721440, train lm loss: 0.8942416538484395
epoch 1, global step 5060, total step 721440, train lm loss: 0.8179991582408548
epoch 1, global step 5070, total step 721440, train lm loss: 0.8775249935686589
epoch 1, global step 5080, total step 721440, train lm loss: 0.8227952203713358
epoch 1, global step 5090, total step 721440, train lm loss: 0.8389456794131547
epoch 1, global step 5100, total step 721440, train lm loss: 0.8618577193468809
epoch 1, global step 5110, total step 721440, train lm loss: 0.8116277159191668
epoch 1, global step 5120, total step 721440, train lm loss: 0.8178052758798003
epoch 1, global step 5130, total step 721440, train lm loss: 0.8813540328294038
epoch 1, global step 5140, total step 721440, train lm loss: 0.8346442709676921
epoch 1, global step 5150, total step 721440, train lm loss: 0.861097875237465
epoch 1, global step 5160, total step 721440, train lm loss: 0.9215585668571293
epoch 1, global step 5170, total step 721440, train lm loss: 0.8402781819924712
epoch 1, global step 5180, total step 721440, train lm loss: 0.8514660802204161
epoch 1, global step 5190, total step 721440, train lm loss: 0.7817759210709483
epoch 1, global step 5200, total step 721440, train lm loss: 0.8274518011137844
epoch 1, global step 5210, total step 721440, train lm loss: 0.833457718975842
epoch 1, global step 5220, total step 721440, train lm loss: 0.8064631181769073
epoch 1, global step 5230, total step 721440, train lm loss: 0.849241189006716
epoch 1, global step 5240, total step 721440, train lm loss: 0.8098228605464101
epoch 1, global step 5250, total step 721440, train lm loss: 0.8259450308047235
epoch 1, global step 5260, total step 721440, train lm loss: 0.854074033908546
epoch 1, global step 5270, total step 721440, train lm loss: 0.8427448300644755
epoch 1, global step 5280, total step 721440, train lm loss: 0.8354785055853426
epoch 1, global step 5290, total step 721440, train lm loss: 0.8638370130211115
epoch 1, global step 5300, total step 721440, train lm loss: 0.8286229175515473
epoch 1, global step 5310, total step 721440, train lm loss: 0.8006645433604718
epoch 1, global step 5320, total step 721440, train lm loss: 0.8338704289868474
epoch 1, global step 5330, total step 721440, train lm loss: 0.8680149442981928
epoch 1, global step 5340, total step 721440, train lm loss: 0.8284092268906533
epoch 1, global step 5350, total step 721440, train lm loss: 0.8761561846360564
epoch 1, global step 5360, total step 721440, train lm loss: 0.8084645710419863
epoch 1, global step 5370, total step 721440, train lm loss: 0.8439460679888725
epoch 1, global step 5380, total step 721440, train lm loss: 0.808971965033561
epoch 1, global step 5390, total step 721440, train lm loss: 0.8456709170714021
epoch 1, global step 5400, total step 721440, train lm loss: 0.8279205000959337
epoch 1, global step 5410, total step 721440, train lm loss: 0.8096932217013091
epoch 1, global step 5420, total step 721440, train lm loss: 0.8401528744958341
epoch 1, global step 5430, total step 721440, train lm loss: 0.839034627424553
epoch 1, global step 5440, total step 721440, train lm loss: 0.7524807026609779
epoch 1, global step 5450, total step 721440, train lm loss: 0.8286147009581327
epoch 1, global step 5460, total step 721440, train lm loss: 0.8124632022343576
epoch 1, global step 5470, total step 721440, train lm loss: 0.8165386460721493
epoch 1, global step 5480, total step 721440, train lm loss: 0.7899630179628729
epoch 1, global step 5490, total step 721440, train lm loss: 0.7602069383487106
epoch 1, global step 5500, total step 721440, train lm loss: 0.8075589157640934
epoch 1, global step 5510, total step 721440, train lm loss: 0.7988435588777065
epoch 1, global step 5520, total step 721440, train lm loss: 0.825819393619895
epoch 1, global step 5530, total step 721440, train lm loss: 0.8120283325901255
epoch 1, global step 5540, total step 721440, train lm loss: 0.8344844965264201
epoch 1, global step 5550, total step 721440, train lm loss: 0.8348154420033097
epoch 1, global step 5560, total step 721440, train lm loss: 0.8437366130761802
epoch 1, global step 5570, total step 721440, train lm loss: 0.8438012252561748
epoch 1, global step 5580, total step 721440, train lm loss: 0.8263016482815146
epoch 1, global step 5590, total step 721440, train lm loss: 0.8215625792741775
epoch 1, global step 5600, total step 721440, train lm loss: 0.8103222883306443
epoch 1, global step 5610, total step 721440, train lm loss: 0.7872189622372389
epoch 1, global step 5620, total step 721440, train lm loss: 0.7832443357910961
epoch 1, global step 5630, total step 721440, train lm loss: 0.8339960872195661
epoch 1, global step 5640, total step 721440, train lm loss: 0.8040012368001044
epoch 1, global step 5650, total step 721440, train lm loss: 0.7631336722522974
epoch 1, global step 5660, total step 721440, train lm loss: 0.7761844351422041
epoch 1, global step 5670, total step 721440, train lm loss: 0.7989891516976059
epoch 1, global step 5680, total step 721440, train lm loss: 0.7540960189886391
epoch 1, global step 5690, total step 721440, train lm loss: 0.7669546900782734
epoch 1, global step 5700, total step 721440, train lm loss: 0.8091930763330311
epoch 1, global step 5710, total step 721440, train lm loss: 0.7781497877091169
epoch 1, global step 5720, total step 721440, train lm loss: 0.8098049682099372
epoch 1, global step 5730, total step 721440, train lm loss: 0.816229793895036
epoch 1, global step 5740, total step 721440, train lm loss: 0.8545799868647009
epoch 1, global step 5750, total step 721440, train lm loss: 0.778133146231994
epoch 1, global step 5760, total step 721440, train lm loss: 0.7927288533188402
epoch 1, global step 5770, total step 721440, train lm loss: 0.7859009988605976
epoch 1, global step 5780, total step 721440, train lm loss: 0.7641893556341529
epoch 1, global step 5790, total step 721440, train lm loss: 0.8129492823034525
epoch 1, global step 5800, total step 721440, train lm loss: 0.7458302761428058
epoch 1, global step 5810, total step 721440, train lm loss: 0.794925007573329
epoch 1, global step 5820, total step 721440, train lm loss: 0.7540083203464747
epoch 1, global step 5830, total step 721440, train lm loss: 0.7852124475874007
epoch 1, global step 5840, total step 721440, train lm loss: 0.7623864311724902
epoch 1, global step 5850, total step 721440, train lm loss: 0.7626217561773956
epoch 1, global step 5860, total step 721440, train lm loss: 0.7900659066159278
epoch 1, global step 5870, total step 721440, train lm loss: 0.7973502545151859
epoch 1, global step 5880, total step 721440, train lm loss: 0.8111083164811135
epoch 1, global step 5890, total step 721440, train lm loss: 0.7378359023481608
epoch 1, global step 5900, total step 721440, train lm loss: 0.7407919369637966
epoch 1, global step 5910, total step 721440, train lm loss: 0.7569585468154401
epoch 1, global step 5920, total step 721440, train lm loss: 0.8074503827840089
epoch 1, global step 5930, total step 721440, train lm loss: 0.7771465258672834
epoch 1, global step 5940, total step 721440, train lm loss: 0.7163579599931836
epoch 1, global step 5950, total step 721440, train lm loss: 0.7503739645704627
epoch 1, global step 5960, total step 721440, train lm loss: 0.7639203905127943
epoch 1, global step 5970, total step 721440, train lm loss: 0.7831930576823651
epoch 1, global step 5980, total step 721440, train lm loss: 0.7514866375364363
epoch 1, global step 5990, total step 721440, train lm loss: 0.8033200103789568
epoch 1, global step 6000, total step 721440, train lm loss: 0.698108589835465
epoch 1, global step 6010, total step 721440, train lm loss: 0.7628202242311091
epoch 1, global step 6020, total step 721440, train lm loss: 0.7452276829164475
epoch 1, global step 6030, total step 721440, train lm loss: 0.7838050670921802
epoch 1, global step 6040, total step 721440, train lm loss: 0.7567944875918329
epoch 1, global step 6050, total step 721440, train lm loss: 0.7976203276775777
epoch 1, global step 6060, total step 721440, train lm loss: 0.7566705408971757
epoch 1, global step 6070, total step 721440, train lm loss: 0.7671165633015334
epoch 1, global step 6080, total step 721440, train lm loss: 0.766136662196368
epoch 1, global step 6090, total step 721440, train lm loss: 0.7528368918690831
epoch 1, global step 6100, total step 721440, train lm loss: 0.7579484150744975
epoch 1, global step 6110, total step 721440, train lm loss: 0.7750846529379487
epoch 1, global step 6120, total step 721440, train lm loss: 0.8142532145604491
epoch 1, global step 6130, total step 721440, train lm loss: 0.7867703915573656
epoch 1, global step 6140, total step 721440, train lm loss: 0.7903715561144053
epoch 1, global step 6150, total step 721440, train lm loss: 0.8125606622546911
epoch 1, global step 6160, total step 721440, train lm loss: 0.7678927826927975
epoch 1, global step 6170, total step 721440, train lm loss: 0.770483891479671
epoch 1, global step 6180, total step 721440, train lm loss: 0.7339925824198872
epoch 1, global step 6190, total step 721440, train lm loss: 0.7350911202840507
epoch 1, global step 6200, total step 721440, train lm loss: 0.7144762780983
epoch 1, global step 6210, total step 721440, train lm loss: 0.7937635510694235
epoch 1, global step 6220, total step 721440, train lm loss: 0.7172356997616589
epoch 1, global step 6230, total step 721440, train lm loss: 0.7576773061882704
epoch 1, global step 6240, total step 721440, train lm loss: 0.7474181841127574
epoch 1, global step 6250, total step 721440, train lm loss: 0.7646863214671612
epoch 1, global step 6260, total step 721440, train lm loss: 0.724006001278758
epoch 1, global step 6270, total step 721440, train lm loss: 0.7629603269044309
epoch 1, global step 6280, total step 721440, train lm loss: 0.7472365577705204
epoch 1, global step 6290, total step 721440, train lm loss: 0.7506064599379897
epoch 1, global step 6300, total step 721440, train lm loss: 0.7219315837137401
epoch 1, global step 6310, total step 721440, train lm loss: 0.7589648865628987
epoch 1, global step 6320, total step 721440, train lm loss: 0.7644054297357797
epoch 1, global step 6330, total step 721440, train lm loss: 0.7685731939040125
epoch 1, global step 6340, total step 721440, train lm loss: 0.7716358778998256
epoch 1, global step 6350, total step 721440, train lm loss: 0.7338014912791551
epoch 1, global step 6360, total step 721440, train lm loss: 0.743155652936548
epoch 1, global step 6370, total step 721440, train lm loss: 0.7909550677053631
epoch 1, global step 6380, total step 721440, train lm loss: 0.7344523290172219
epoch 1, global step 6390, total step 721440, train lm loss: 0.7598110746592284
epoch 1, global step 6400, total step 721440, train lm loss: 0.72038664361462
epoch 1, global step 6410, total step 721440, train lm loss: 0.7412419324740768
epoch 1, global step 6420, total step 721440, train lm loss: 0.7462146854959428
epoch 1, global step 6430, total step 721440, train lm loss: 0.7643207737244666
epoch 1, global step 6440, total step 721440, train lm loss: 0.7710155153181404
epoch 1, global step 6450, total step 721440, train lm loss: 0.7762257111724467
epoch 1, global step 6460, total step 721440, train lm loss: 0.7488167046569287
epoch 1, global step 6470, total step 721440, train lm loss: 0.7057362597901374
epoch 1, global step 6480, total step 721440, train lm loss: 0.7400911565870046
epoch 1, global step 6490, total step 721440, train lm loss: 0.7672552130185067
epoch 1, global step 6500, total step 721440, train lm loss: 0.7423301464878023
epoch 1, global step 6510, total step 721440, train lm loss: 0.7157708851154894
epoch 1, global step 6520, total step 721440, train lm loss: 0.7662325616460294
epoch 1, global step 6530, total step 721440, train lm loss: 0.7418296338059008
epoch 1, global step 6540, total step 721440, train lm loss: 0.7425602335482836
epoch 1, global step 6550, total step 721440, train lm loss: 0.742135436180979
epoch 1, global step 6560, total step 721440, train lm loss: 0.7581525928340852
epoch 1, global step 6570, total step 721440, train lm loss: 0.7061773061752319
epoch 1, global step 6580, total step 721440, train lm loss: 0.6994570296257734
epoch 1, global step 6590, total step 721440, train lm loss: 0.7013044382445515
epoch 1, global step 6600, total step 721440, train lm loss: 0.7241010350175202
epoch 1, global step 6610, total step 721440, train lm loss: 0.6996444867458195
epoch 1, global step 6620, total step 721440, train lm loss: 0.7741003955714405
epoch 1, global step 6630, total step 721440, train lm loss: 0.7149799801409245
epoch 1, global step 6640, total step 721440, train lm loss: 0.7253318874631077
epoch 1, global step 6650, total step 721440, train lm loss: 0.7200759914703667
epoch 1, global step 6660, total step 721440, train lm loss: 0.6976395062170923
epoch 1, global step 6670, total step 721440, train lm loss: 0.74813211876899
epoch 1, global step 6680, total step 721440, train lm loss: 0.753603000799194
epoch 1, global step 6690, total step 721440, train lm loss: 0.7289167053531855
epoch 1, global step 6700, total step 721440, train lm loss: 0.7248279371764511
epoch 1, global step 6710, total step 721440, train lm loss: 0.6894724206067622
epoch 1, global step 6720, total step 721440, train lm loss: 0.6905436322558671
epoch 1, global step 6730, total step 721440, train lm loss: 0.7034089244436472
epoch 1, global step 6740, total step 721440, train lm loss: 0.6863873207941651
epoch 1, global step 6750, total step 721440, train lm loss: 0.6737910561263561
epoch 1, global step 6760, total step 721440, train lm loss: 0.7819312032777817
epoch 1, global step 6770, total step 721440, train lm loss: 0.7160888771526516
epoch 1, global step 6780, total step 721440, train lm loss: 0.7164466949179769
epoch 1, global step 6790, total step 721440, train lm loss: 0.7168825432658196
epoch 1, global step 6800, total step 721440, train lm loss: 0.730500343395397
epoch 1, global step 6810, total step 721440, train lm loss: 0.7195645041298121
epoch 1, global step 6820, total step 721440, train lm loss: 0.7043594713788479
epoch 1, global step 6830, total step 721440, train lm loss: 0.7210937352851033
epoch 1, global step 6840, total step 721440, train lm loss: 0.6579757351893931
epoch 1, global step 6850, total step 721440, train lm loss: 0.7246992896310985
epoch 1, global step 6860, total step 721440, train lm loss: 0.6992668082006276
epoch 1, global step 6870, total step 721440, train lm loss: 0.7754393028095364
epoch 1, global step 6880, total step 721440, train lm loss: 0.695983942784369
epoch 1, global step 6890, total step 721440, train lm loss: 0.6758702888153494
epoch 1, global step 6900, total step 721440, train lm loss: 0.7037800252437592
epoch 1, global step 6910, total step 721440, train lm loss: 0.6854314631782472
epoch 1, global step 6920, total step 721440, train lm loss: 0.6635107284411788
epoch 1, global step 6930, total step 721440, train lm loss: 0.699561885651201
epoch 1, global step 6940, total step 721440, train lm loss: 0.7137197947595268
epoch 1, global step 6950, total step 721440, train lm loss: 0.7003981626592577
epoch 1, global step 6960, total step 721440, train lm loss: 0.6585073709487915
epoch 1, global step 6970, total step 721440, train lm loss: 0.6312466789968312
epoch 1, global step 6980, total step 721440, train lm loss: 0.6853253585752099
epoch 1, global step 6990, total step 721440, train lm loss: 0.62580079510808
epoch 1, global step 7000, total step 721440, train lm loss: 0.665198135515675
epoch 1, global step 7010, total step 721440, train lm loss: 0.704811160126701
epoch 1, global step 7020, total step 721440, train lm loss: 0.7025526027195156
epoch 1, global step 7030, total step 721440, train lm loss: 0.7245652948273346
epoch 1, global step 7040, total step 721440, train lm loss: 0.6931011754553765
epoch 1, global step 7050, total step 721440, train lm loss: 0.6860738057177513
epoch 1, global step 7060, total step 721440, train lm loss: 0.7077390119433403
epoch 1, global step 7070, total step 721440, train lm loss: 0.6649117863737046
epoch 1, global step 7080, total step 721440, train lm loss: 0.6873433058150112
epoch 1, global step 7090, total step 721440, train lm loss: 0.6880880762124434
epoch 1, global step 7100, total step 721440, train lm loss: 0.7519905704073608
epoch 1, global step 7110, total step 721440, train lm loss: 0.6940692347940057
epoch 1, global step 7120, total step 721440, train lm loss: 0.718823977932334
epoch 1, global step 7130, total step 721440, train lm loss: 0.6639618061482906
epoch 1, global step 7140, total step 721440, train lm loss: 0.6554239262826741
epoch 1, global step 7150, total step 721440, train lm loss: 0.6647499982733279
epoch 1, global step 7160, total step 721440, train lm loss: 0.6718457338865846
epoch 1, global step 7170, total step 721440, train lm loss: 0.7147620797157288
epoch 1, global step 7180, total step 721440, train lm loss: 0.7257128824479878
epoch 1, global step 7190, total step 721440, train lm loss: 0.6614323561545461
epoch 1, global step 7200, total step 721440, train lm loss: 0.6750246026553214
epoch 1, global step 7210, total step 721440, train lm loss: 0.6562012694310397
epoch 1, global step 7220, total step 721440, train lm loss: 0.7089578720740974
epoch 1, global step 7230, total step 721440, train lm loss: 0.6582103020045906
epoch 1, global step 7240, total step 721440, train lm loss: 0.6679582887329161
epoch 1, global step 7250, total step 721440, train lm loss: 0.6763461284805089
epoch 1, global step 7260, total step 721440, train lm loss: 0.6734707536641509
epoch 1, global step 7270, total step 721440, train lm loss: 0.6249100126558915
epoch 1, global step 7280, total step 721440, train lm loss: 0.6849540121853351
epoch 1, global step 7290, total step 721440, train lm loss: 0.6744251534342766
epoch 1, global step 7300, total step 721440, train lm loss: 0.6834950145334006
epoch 1, global step 7310, total step 721440, train lm loss: 0.6172097075264901
epoch 1, global step 7320, total step 721440, train lm loss: 0.6584702975582332
epoch 1, global step 7330, total step 721440, train lm loss: 0.6624065499287098
epoch 1, global step 7340, total step 721440, train lm loss: 0.6771353339776397
epoch 1, global step 7350, total step 721440, train lm loss: 0.6640137183014303
epoch 1, global step 7360, total step 721440, train lm loss: 0.6949419343378395
epoch 1, global step 7370, total step 721440, train lm loss: 0.6189411984523758
epoch 1, global step 7380, total step 721440, train lm loss: 0.6299032422248274
epoch 1, global step 7390, total step 721440, train lm loss: 0.6453833873383701
epoch 1, global step 7400, total step 721440, train lm loss: 0.6564256225712597
epoch 1, global step 7410, total step 721440, train lm loss: 0.6323058284819126
epoch 1, global step 7420, total step 721440, train lm loss: 0.5752600899431854
epoch 1, global step 7430, total step 721440, train lm loss: 0.672511309524998
epoch 1, global step 7440, total step 721440, train lm loss: 0.6313468940090388
epoch 1, global step 7450, total step 721440, train lm loss: 0.655554233957082
epoch 1, global step 7460, total step 721440, train lm loss: 0.670347234653309
epoch 1, global step 7470, total step 721440, train lm loss: 0.6601198620861396
epoch 1, global step 7480, total step 721440, train lm loss: 0.6702698894310742
epoch 1, global step 7490, total step 721440, train lm loss: 0.6980067178606987
epoch 1, global step 7500, total step 721440, train lm loss: 0.6354820324108005
epoch 1, global step 7510, total step 721440, train lm loss: 0.7109196033794433
epoch 1, global step 7520, total step 721440, train lm loss: 0.616020537330769
epoch 1, global step 7530, total step 721440, train lm loss: 0.6138642612146213
epoch 1, global step 7540, total step 721440, train lm loss: 0.669216784229502
epoch 1, global step 7550, total step 721440, train lm loss: 0.6144730007275939
epoch 1, global step 7560, total step 721440, train lm loss: 0.6889130644965917
epoch 1, global step 7570, total step 721440, train lm loss: 0.6420826379675418
epoch 1, global step 7580, total step 721440, train lm loss: 0.6487667056266219
epoch 1, global step 7590, total step 721440, train lm loss: 0.681433531595394
epoch 1, global step 7600, total step 721440, train lm loss: 0.6563435181975364
epoch 1, global step 7610, total step 721440, train lm loss: 0.6307748298160731
epoch 1, global step 7620, total step 721440, train lm loss: 0.7071135453879833
epoch 1, global step 7630, total step 721440, train lm loss: 0.5967203070875258
epoch 1, global step 7640, total step 721440, train lm loss: 0.6219251157715917
epoch 1, global step 7650, total step 721440, train lm loss: 0.6399980176240205
epoch 1, global step 7660, total step 721440, train lm loss: 0.6094263188540936
epoch 1, global step 7670, total step 721440, train lm loss: 0.6250590957002714
epoch 1, global step 7680, total step 721440, train lm loss: 0.6024849798996001
epoch 1, global step 7690, total step 721440, train lm loss: 0.6407933015376329
epoch 1, global step 7700, total step 721440, train lm loss: 0.628911191294901
epoch 1, global step 7710, total step 721440, train lm loss: 0.6688727183733135
epoch 1, global step 7720, total step 721440, train lm loss: 0.6629728472325951
epoch 1, global step 7730, total step 721440, train lm loss: 0.6582805873360484
epoch 1, global step 7740, total step 721440, train lm loss: 0.6365448142401874
epoch 1, global step 7750, total step 721440, train lm loss: 0.5955018136184662
epoch 1, global step 7760, total step 721440, train lm loss: 0.6259049347834662
epoch 1, global step 7770, total step 721440, train lm loss: 0.6119202516973019
epoch 1, global step 7780, total step 721440, train lm loss: 0.6334002563962713
epoch 1, global step 7790, total step 721440, train lm loss: 0.6360896958969533
epoch 1, global step 7800, total step 721440, train lm loss: 0.6618663778528571
epoch 1, global step 7810, total step 721440, train lm loss: 0.6185555252479389
epoch 1, global step 7820, total step 721440, train lm loss: 0.5689631313318386
epoch 1, global step 7830, total step 721440, train lm loss: 0.592718014633283
epoch 1, global step 7840, total step 721440, train lm loss: 0.7221758012194186
epoch 1, global step 7850, total step 721440, train lm loss: 0.6396506155841053
epoch 1, global step 7860, total step 721440, train lm loss: 0.6145947694545612
epoch 1, global step 7870, total step 721440, train lm loss: 0.6510932405013591
epoch 1, global step 7880, total step 721440, train lm loss: 0.7058598274830729
epoch 1, global step 7890, total step 721440, train lm loss: 0.6507294720504433
epoch 1, global step 7900, total step 721440, train lm loss: 0.6123235050123185
epoch 1, global step 7910, total step 721440, train lm loss: 0.6284196233842522
epoch 1, global step 7920, total step 721440, train lm loss: 0.6426900925580412
epoch 1, global step 7930, total step 721440, train lm loss: 0.6221300923731178
epoch 1, global step 7940, total step 721440, train lm loss: 0.6397391302045434
epoch 1, global step 7950, total step 721440, train lm loss: 0.5978624818380922
epoch 1, global step 7960, total step 721440, train lm loss: 0.6260983938816935
epoch 1, global step 7970, total step 721440, train lm loss: 0.6409684795886278
epoch 1, global step 7980, total step 721440, train lm loss: 0.6141300097922795
epoch 1, global step 7990, total step 721440, train lm loss: 0.6168157922104001
epoch 1, global step 8000, total step 721440, train lm loss: 0.5933786087436601
epoch 1, global step 8010, total step 721440, train lm loss: 0.6167867841199041
epoch 1, global step 8020, total step 721440, train lm loss: 0.6446231778245419
epoch 1, global step 8030, total step 721440, train lm loss: 0.6212980487383902
epoch 1, global step 8040, total step 721440, train lm loss: 0.6176770562771707
epoch 1, global step 8050, total step 721440, train lm loss: 0.5664148328476586
epoch 1, global step 8060, total step 721440, train lm loss: 0.6044162474572659
epoch 1, global step 8070, total step 721440, train lm loss: 0.6261062789941206
epoch 1, global step 8080, total step 721440, train lm loss: 0.6677809042390436
epoch 1, global step 8090, total step 721440, train lm loss: 0.6317743437364698
epoch 1, global step 8100, total step 721440, train lm loss: 0.6226189584936946
epoch 1, global step 8110, total step 721440, train lm loss: 0.6079501747153699
epoch 1, global step 8120, total step 721440, train lm loss: 0.5467580245342105
epoch 1, global step 8130, total step 721440, train lm loss: 0.5907051503658295
epoch 1, global step 8140, total step 721440, train lm loss: 0.6691334339790046
epoch 1, global step 8150, total step 721440, train lm loss: 0.6215742530766875
epoch 1, global step 8160, total step 721440, train lm loss: 0.6440962854772806
epoch 1, global step 8170, total step 721440, train lm loss: 0.583581499569118
epoch 1, global step 8180, total step 721440, train lm loss: 0.5887499824166298
epoch 1, global step 8190, total step 721440, train lm loss: 0.558776035066694
epoch 1, global step 8200, total step 721440, train lm loss: 0.6223919334355742
epoch 1, global step 8210, total step 721440, train lm loss: 0.5799896291224286
epoch 1, global step 8220, total step 721440, train lm loss: 0.5980621371301822
epoch 1, global step 8230, total step 721440, train lm loss: 0.6151890984503552
epoch 1, global step 8240, total step 721440, train lm loss: 0.6586857032030821
epoch 1, global step 8250, total step 721440, train lm loss: 0.5546225641388446
epoch 1, global step 8260, total step 721440, train lm loss: 0.610039875539951
epoch 1, global step 8270, total step 721440, train lm loss: 0.5899357852060347
epoch 1, global step 8280, total step 721440, train lm loss: 0.6399962636176497
epoch 1, global step 8290, total step 721440, train lm loss: 0.5494600921403616
epoch 1, global step 8300, total step 721440, train lm loss: 0.591391083272174
epoch 1, global step 8310, total step 721440, train lm loss: 0.6013295548036695
epoch 1, global step 8320, total step 721440, train lm loss: 0.5984088411089032
epoch 1, global step 8330, total step 721440, train lm loss: 0.5648456887807697
epoch 1, global step 8340, total step 721440, train lm loss: 0.5571354963816703
epoch 1, global step 8350, total step 721440, train lm loss: 0.5829169891308993
epoch 1, global step 8360, total step 721440, train lm loss: 0.5813800147734582
epoch 1, global step 8370, total step 721440, train lm loss: 0.6152343404013664
epoch 1, global step 8380, total step 721440, train lm loss: 0.602110523916781
epoch 1, global step 8390, total step 721440, train lm loss: 0.5510928387986496
epoch 1, global step 8400, total step 721440, train lm loss: 0.5936664276523516
epoch 1, global step 8410, total step 721440, train lm loss: 0.6003334710840136
epoch 1, global step 8420, total step 721440, train lm loss: 0.5275061961030587
epoch 1, global step 8430, total step 721440, train lm loss: 0.5897390217985958
epoch 1, global step 8440, total step 721440, train lm loss: 0.5626233923481777
epoch 1, global step 8450, total step 721440, train lm loss: 0.6034764975076541
epoch 1, global step 8460, total step 721440, train lm loss: 0.5997990800300613
epoch 1, global step 8470, total step 721440, train lm loss: 0.5808202860411257
epoch 1, global step 8480, total step 721440, train lm loss: 0.5817898293957114
epoch 1, global step 8490, total step 721440, train lm loss: 0.5399379941634834
epoch 1, global step 8500, total step 721440, train lm loss: 0.5947866555303335
epoch 1, global step 8510, total step 721440, train lm loss: 0.5902020135661588
epoch 1, global step 8520, total step 721440, train lm loss: 0.5625721035525203
epoch 1, global step 8530, total step 721440, train lm loss: 0.5839026660076343
epoch 1, global step 8540, total step 721440, train lm loss: 0.6056353095453233
epoch 1, global step 8550, total step 721440, train lm loss: 0.5627720489399508
epoch 1, global step 8560, total step 721440, train lm loss: 0.6095461091957987
epoch 1, global step 8570, total step 721440, train lm loss: 0.5717244125902653
epoch 1, global step 8580, total step 721440, train lm loss: 0.5505856488831341
epoch 1, global step 8590, total step 721440, train lm loss: 0.5993073606397956
epoch 1, global step 8600, total step 721440, train lm loss: 0.5593529051635414
epoch 1, global step 8610, total step 721440, train lm loss: 0.5310269794194028
epoch 1, global step 8620, total step 721440, train lm loss: 0.5554378467029892
epoch 1, global step 8630, total step 721440, train lm loss: 0.5690439555328339
epoch 1, global step 8640, total step 721440, train lm loss: 0.5374212067574262
epoch 1, global step 8650, total step 721440, train lm loss: 0.5731662428705022
epoch 1, global step 8660, total step 721440, train lm loss: 0.6366941718035377
epoch 1, global step 8670, total step 721440, train lm loss: 0.5892026609391905
epoch 1, global step 8680, total step 721440, train lm loss: 0.6075561515521258
epoch 1, global step 8690, total step 721440, train lm loss: 0.569003876671195
epoch 1, global step 8700, total step 721440, train lm loss: 0.5947650189045817
epoch 1, global step 8710, total step 721440, train lm loss: 0.6002232312224806
epoch 1, global step 8720, total step 721440, train lm loss: 0.5497585553210229
epoch 1, global step 8730, total step 721440, train lm loss: 0.5530567744746804
epoch 1, global step 8740, total step 721440, train lm loss: 0.5478323347866535
epoch 1, global step 8750, total step 721440, train lm loss: 0.538362669851631
epoch 1, global step 8760, total step 721440, train lm loss: 0.555740479263477
epoch 1, global step 8770, total step 721440, train lm loss: 0.6244739175774157
epoch 1, global step 8780, total step 721440, train lm loss: 0.5688206344377249
epoch 1, global step 8790, total step 721440, train lm loss: 0.528693109471351
epoch 1, global step 8800, total step 721440, train lm loss: 0.5882619841024279
epoch 1, global step 8810, total step 721440, train lm loss: 0.565163235925138
epoch 1, global step 8820, total step 721440, train lm loss: 0.5830111921881326
epoch 1, global step 8830, total step 721440, train lm loss: 0.5738934886176139
epoch 1, global step 8840, total step 721440, train lm loss: 0.5146200729301199
epoch 1, global step 8850, total step 721440, train lm loss: 0.5560932133812457
epoch 1, global step 8860, total step 721440, train lm loss: 0.5901980219990947
epoch 1, global step 8870, total step 721440, train lm loss: 0.5496093358378857
epoch 1, global step 8880, total step 721440, train lm loss: 0.5979985324200243
epoch 1, global step 8890, total step 721440, train lm loss: 0.5159419079311192
epoch 1, global step 8900, total step 721440, train lm loss: 0.5337685208767653
epoch 1, global step 8910, total step 721440, train lm loss: 0.5254412971902639
epoch 1, global step 8920, total step 721440, train lm loss: 0.5178522845264524
epoch 1, global step 8930, total step 721440, train lm loss: 0.5256154586095363
epoch 1, global step 8940, total step 721440, train lm loss: 0.5419750213157386
epoch 1, global step 8950, total step 721440, train lm loss: 0.5289949706289917
epoch 1, global step 8960, total step 721440, train lm loss: 0.5650810767663643
epoch 1, global step 8970, total step 721440, train lm loss: 0.5831284524640068
epoch 1, global step 8980, total step 721440, train lm loss: 0.5475312024354935
epoch 1, global step 8990, total step 721440, train lm loss: 0.5327977922512218
epoch 1, global step 9000, total step 721440, train lm loss: 0.5032896341290325
epoch 1, global step 9010, total step 721440, train lm loss: 0.4877197925699875
epoch 2, global step 9020, total step 721440, train lm loss: 0.5725049152970314
epoch 2, global step 9030, total step 721440, train lm loss: 0.5756633450393565
epoch 2, global step 9040, total step 721440, train lm loss: 0.5549916073447093
epoch 2, global step 9050, total step 721440, train lm loss: 0.561284065619111
epoch 2, global step 9060, total step 721440, train lm loss: 0.5083899924124126
epoch 2, global step 9070, total step 721440, train lm loss: 0.5261461342219264
epoch 2, global step 9080, total step 721440, train lm loss: 0.6017535337712616
epoch 2, global step 9090, total step 721440, train lm loss: 0.532563847862184
epoch 2, global step 9100, total step 721440, train lm loss: 0.5661891972180456
epoch 2, global step 9110, total step 721440, train lm loss: 0.5183822802500799
epoch 2, global step 9120, total step 721440, train lm loss: 0.5572390050627292
epoch 2, global step 9130, total step 721440, train lm loss: 0.5799502857495099
epoch 2, global step 9140, total step 721440, train lm loss: 0.5466854080324992
epoch 2, global step 9150, total step 721440, train lm loss: 0.5324002649635077
epoch 2, global step 9160, total step 721440, train lm loss: 0.5451672134222463
epoch 2, global step 9170, total step 721440, train lm loss: 0.5602827163529582
epoch 2, global step 9180, total step 721440, train lm loss: 0.504799968667794
epoch 2, global step 9190, total step 721440, train lm loss: 0.49922876970376817
epoch 2, global step 9200, total step 721440, train lm loss: 0.5091214816551656
epoch 2, global step 9210, total step 721440, train lm loss: 0.48976578093133866
epoch 2, global step 9220, total step 721440, train lm loss: 0.5074735853355378
epoch 2, global step 9230, total step 721440, train lm loss: 0.5105604959186166
epoch 2, global step 9240, total step 721440, train lm loss: 0.5508312735240907
epoch 2, global step 9250, total step 721440, train lm loss: 0.5250954537652432
epoch 2, global step 9260, total step 721440, train lm loss: 0.5250492733903229
epoch 2, global step 9270, total step 721440, train lm loss: 0.5715290278429166
epoch 2, global step 9280, total step 721440, train lm loss: 0.4944562064949423
epoch 2, global step 9290, total step 721440, train lm loss: 0.5253713839803822
epoch 2, global step 9300, total step 721440, train lm loss: 0.4866280548274517
epoch 2, global step 9310, total step 721440, train lm loss: 0.5210233375895769
epoch 2, global step 9320, total step 721440, train lm loss: 0.5469858647789806
epoch 2, global step 9330, total step 721440, train lm loss: 0.5710603023529984
epoch 2, global step 9340, total step 721440, train lm loss: 0.5121042255312205
epoch 2, global step 9350, total step 721440, train lm loss: 0.5629662771942094
epoch 2, global step 9360, total step 721440, train lm loss: 0.5398918056627735
epoch 2, global step 9370, total step 721440, train lm loss: 0.5296320476569235
epoch 2, global step 9380, total step 721440, train lm loss: 0.5158146569272504
epoch 2, global step 9390, total step 721440, train lm loss: 0.5218406908214093
epoch 2, global step 9400, total step 721440, train lm loss: 0.560668276832439
epoch 2, global step 9410, total step 721440, train lm loss: 0.5118869670201093
epoch 2, global step 9420, total step 721440, train lm loss: 0.5052970835240558
epoch 2, global step 9430, total step 721440, train lm loss: 0.4934681677201297
epoch 2, global step 9440, total step 721440, train lm loss: 0.5089592257514596
epoch 2, global step 9450, total step 721440, train lm loss: 0.5102955056587234
epoch 2, global step 9460, total step 721440, train lm loss: 0.5033560582902282
epoch 2, global step 9470, total step 721440, train lm loss: 0.48560319216921927
epoch 2, global step 9480, total step 721440, train lm loss: 0.4878376379609108
epoch 2, global step 9490, total step 721440, train lm loss: 0.5249121825909242
epoch 2, global step 9500, total step 721440, train lm loss: 0.48268356594489886
epoch 2, global step 9510, total step 721440, train lm loss: 0.5795778553932905
epoch 2, global step 9520, total step 721440, train lm loss: 0.5509882527170703
epoch 2, global step 9530, total step 721440, train lm loss: 0.5320500839850866
epoch 2, global step 9540, total step 721440, train lm loss: 0.5285088943317533
epoch 2, global step 9550, total step 721440, train lm loss: 0.4873347743647173
epoch 2, global step 9560, total step 721440, train lm loss: 0.5118842627736739
epoch 2, global step 9570, total step 721440, train lm loss: 0.44857939120847734
epoch 2, global step 9580, total step 721440, train lm loss: 0.5764651402132586
epoch 2, global step 9590, total step 721440, train lm loss: 0.5161404696526006
epoch 2, global step 9600, total step 721440, train lm loss: 0.5079432708211243
epoch 2, global step 9610, total step 721440, train lm loss: 0.5268593586748466
epoch 2, global step 9620, total step 721440, train lm loss: 0.5020987508818507
epoch 2, global step 9630, total step 721440, train lm loss: 0.5181106635835022
epoch 2, global step 9640, total step 721440, train lm loss: 0.5452272344380618
epoch 2, global step 9650, total step 721440, train lm loss: 0.549318319093436
epoch 2, global step 9660, total step 721440, train lm loss: 0.547532334481366
epoch 2, global step 9670, total step 721440, train lm loss: 0.536305767018348
epoch 2, global step 9680, total step 721440, train lm loss: 0.5471143875736744
epoch 2, global step 9690, total step 721440, train lm loss: 0.49782571983523666
epoch 2, global step 9700, total step 721440, train lm loss: 0.47894550336059183
epoch 2, global step 9710, total step 721440, train lm loss: 0.5220050201052799
epoch 2, global step 9720, total step 721440, train lm loss: 0.5145981534384191
epoch 2, global step 9730, total step 721440, train lm loss: 0.4980763897765428
epoch 2, global step 9740, total step 721440, train lm loss: 0.5073808352462947
epoch 2, global step 9750, total step 721440, train lm loss: 0.4965743555803783
epoch 2, global step 9760, total step 721440, train lm loss: 0.5130892745219171
epoch 2, global step 9770, total step 721440, train lm loss: 0.4739938832586631
epoch 2, global step 9780, total step 721440, train lm loss: 0.5651030918583274
epoch 2, global step 9790, total step 721440, train lm loss: 0.5400687928195111
epoch 2, global step 9800, total step 721440, train lm loss: 0.5387647232506424
epoch 2, global step 9810, total step 721440, train lm loss: 0.5335584442829713
epoch 2, global step 9820, total step 721440, train lm loss: 0.44540324389236047
epoch 2, global step 9830, total step 721440, train lm loss: 0.5060898688388988
epoch 2, global step 9840, total step 721440, train lm loss: 0.5147617597132921
epoch 2, global step 9850, total step 721440, train lm loss: 0.5032049936940893
epoch 2, global step 9860, total step 721440, train lm loss: 0.5278655260102824
epoch 2, global step 9870, total step 721440, train lm loss: 0.5264052572078072
epoch 2, global step 9880, total step 721440, train lm loss: 0.5364335553953424
epoch 2, global step 9890, total step 721440, train lm loss: 0.49533045052085073
epoch 2, global step 9900, total step 721440, train lm loss: 0.47527645495720205
epoch 2, global step 9910, total step 721440, train lm loss: 0.4928443091572262
epoch 2, global step 9920, total step 721440, train lm loss: 0.5002286481903866
epoch 2, global step 9930, total step 721440, train lm loss: 0.498646094603464
epoch 2, global step 9940, total step 721440, train lm loss: 0.5097235676832497
epoch 2, global step 9950, total step 721440, train lm loss: 0.4468876058468595
epoch 2, global step 9960, total step 721440, train lm loss: 0.5073591058782767
epoch 2, global step 9970, total step 721440, train lm loss: 0.507703579752706
epoch 2, global step 9980, total step 721440, train lm loss: 0.4896869723103009
epoch 2, global step 9990, total step 721440, train lm loss: 0.49649609050247817
epoch 2, global step 10000, total step 721440, train lm loss: 0.4619339008932002
epoch 2, global step 10010, total step 721440, train lm loss: 0.5207689310889692
epoch 2, global step 10020, total step 721440, train lm loss: 0.4599225810263306
epoch 2, global step 10030, total step 721440, train lm loss: 0.5218771935789845
epoch 2, global step 10040, total step 721440, train lm loss: 0.48345255027525125
epoch 2, global step 10050, total step 721440, train lm loss: 0.493738669808954
epoch 2, global step 10060, total step 721440, train lm loss: 0.5104846697067842
epoch 2, global step 10070, total step 721440, train lm loss: 0.5084019878180698
epoch 2, global step 10080, total step 721440, train lm loss: 0.5329726484138518
epoch 2, global step 10090, total step 721440, train lm loss: 0.5025548728881404
epoch 2, global step 10100, total step 721440, train lm loss: 0.5388869262416847
epoch 2, global step 10110, total step 721440, train lm loss: 0.5304487419547513
epoch 2, global step 10120, total step 721440, train lm loss: 0.5029821980046109
epoch 2, global step 10130, total step 721440, train lm loss: 0.4827948972117156
epoch 2, global step 10140, total step 721440, train lm loss: 0.5449243055423721
epoch 2, global step 10150, total step 721440, train lm loss: 0.5117524011060596
epoch 2, global step 10160, total step 721440, train lm loss: 0.4770976820262149
epoch 2, global step 10170, total step 721440, train lm loss: 0.5066347728017717
epoch 2, global step 10180, total step 721440, train lm loss: 0.49788653349969536
epoch 2, global step 10190, total step 721440, train lm loss: 0.4624775172676891
epoch 2, global step 10200, total step 721440, train lm loss: 0.43616381734609605
epoch 2, global step 10210, total step 721440, train lm loss: 0.47281406591646374
epoch 2, global step 10220, total step 721440, train lm loss: 0.45681955855106937
epoch 2, global step 10230, total step 721440, train lm loss: 0.5085947082377971
epoch 2, global step 10240, total step 721440, train lm loss: 0.5160342676274012
epoch 2, global step 10250, total step 721440, train lm loss: 0.5371883579529821
epoch 2, global step 10260, total step 721440, train lm loss: 0.44064041967503725
epoch 2, global step 10270, total step 721440, train lm loss: 0.5108797378226881
epoch 2, global step 10280, total step 721440, train lm loss: 0.4925591742619872
epoch 2, global step 10290, total step 721440, train lm loss: 0.48048272568266837
epoch 2, global step 10300, total step 721440, train lm loss: 0.45906826625578107
epoch 2, global step 10310, total step 721440, train lm loss: 0.44295727602439
epoch 2, global step 10320, total step 721440, train lm loss: 0.4948160816216841
epoch 2, global step 10330, total step 721440, train lm loss: 0.43382653426378964
epoch 2, global step 10340, total step 721440, train lm loss: 0.5047963978024199
epoch 2, global step 10350, total step 721440, train lm loss: 0.48824596135527826
epoch 2, global step 10360, total step 721440, train lm loss: 0.46834547591861336
epoch 2, global step 10370, total step 721440, train lm loss: 0.4648244427284226
epoch 2, global step 10380, total step 721440, train lm loss: 0.4843541998183355
epoch 2, global step 10390, total step 721440, train lm loss: 0.5065048448741436
epoch 2, global step 10400, total step 721440, train lm loss: 0.45037767752073704
epoch 2, global step 10410, total step 721440, train lm loss: 0.4610275301150978
epoch 2, global step 10420, total step 721440, train lm loss: 0.5033785021398216
epoch 2, global step 10430, total step 721440, train lm loss: 0.48015452744439246
epoch 2, global step 10440, total step 721440, train lm loss: 0.43514956631697715
epoch 2, global step 10450, total step 721440, train lm loss: 0.4430145294929389
epoch 2, global step 10460, total step 721440, train lm loss: 0.46756723234429953
epoch 2, global step 10470, total step 721440, train lm loss: 0.46753207691945137
epoch 2, global step 10480, total step 721440, train lm loss: 0.45207538253162055
epoch 2, global step 10490, total step 721440, train lm loss: 0.48467232334660365
epoch 2, global step 10500, total step 721440, train lm loss: 0.5233122849138454
epoch 2, global step 10510, total step 721440, train lm loss: 0.43464022226398813
epoch 2, global step 10520, total step 721440, train lm loss: 0.478017907962203
epoch 2, global step 10530, total step 721440, train lm loss: 0.47030391639564184
epoch 2, global step 10540, total step 721440, train lm loss: 0.4797338495205622
epoch 2, global step 10550, total step 721440, train lm loss: 0.4323689637938514
epoch 2, global step 10560, total step 721440, train lm loss: 0.4792963471263647
epoch 2, global step 10570, total step 721440, train lm loss: 0.5213607709971256
epoch 2, global step 10580, total step 721440, train lm loss: 0.49681072058156134
epoch 2, global step 10590, total step 721440, train lm loss: 0.46772429151460526
epoch 2, global step 10600, total step 721440, train lm loss: 0.47292291207704695
epoch 2, global step 10610, total step 721440, train lm loss: 0.483777527930215
epoch 2, global step 10620, total step 721440, train lm loss: 0.5053589766845107
epoch 2, global step 10630, total step 721440, train lm loss: 0.5036687540821732
epoch 2, global step 10640, total step 721440, train lm loss: 0.550867788749747
epoch 2, global step 10650, total step 721440, train lm loss: 0.4894155608257279
epoch 2, global step 10660, total step 721440, train lm loss: 0.5081630565458909
epoch 2, global step 10670, total step 721440, train lm loss: 0.47482024920755067
epoch 2, global step 10680, total step 721440, train lm loss: 0.49560177330859007
epoch 2, global step 10690, total step 721440, train lm loss: 0.43605726123787464
epoch 2, global step 10700, total step 721440, train lm loss: 0.47370933372876606
epoch 2, global step 10710, total step 721440, train lm loss: 0.43895546046551315
epoch 2, global step 10720, total step 721440, train lm loss: 0.47364351467695087
epoch 2, global step 10730, total step 721440, train lm loss: 0.4330460809986107
epoch 2, global step 10740, total step 721440, train lm loss: 0.4781445971922949
epoch 2, global step 10750, total step 721440, train lm loss: 0.4560716348001733
epoch 2, global step 10760, total step 721440, train lm loss: 0.4375199515605345
epoch 2, global step 10770, total step 721440, train lm loss: 0.44617287495639174
epoch 2, global step 10780, total step 721440, train lm loss: 0.430304539273493
epoch 2, global step 10790, total step 721440, train lm loss: 0.47391505287960173
epoch 2, global step 10800, total step 721440, train lm loss: 0.4509402292314917
epoch 2, global step 10810, total step 721440, train lm loss: 0.4564180453773588
epoch 2, global step 10820, total step 721440, train lm loss: 0.49202058506198226
epoch 2, global step 10830, total step 721440, train lm loss: 0.4847373356577009
epoch 2, global step 10840, total step 721440, train lm loss: 0.4572682847152464
epoch 2, global step 10850, total step 721440, train lm loss: 0.4792122392449528
epoch 2, global step 10860, total step 721440, train lm loss: 0.4724109276081435
epoch 2, global step 10870, total step 721440, train lm loss: 0.4704026832478121
epoch 2, global step 10880, total step 721440, train lm loss: 0.5178151523461565
epoch 2, global step 10890, total step 721440, train lm loss: 0.4868717370787635
epoch 2, global step 10900, total step 721440, train lm loss: 0.45499855739763007
epoch 2, global step 10910, total step 721440, train lm loss: 0.41625304822809994
epoch 2, global step 10920, total step 721440, train lm loss: 0.4800608470104635
epoch 2, global step 10930, total step 721440, train lm loss: 0.49292335549835115
epoch 2, global step 10940, total step 721440, train lm loss: 0.4722222549375147
epoch 2, global step 10950, total step 721440, train lm loss: 0.473842754913494
epoch 2, global step 10960, total step 721440, train lm loss: 0.5045793064171449
epoch 2, global step 10970, total step 721440, train lm loss: 0.4349714108742774
epoch 2, global step 10980, total step 721440, train lm loss: 0.4653863606508821
epoch 2, global step 10990, total step 721440, train lm loss: 0.459566819074098
epoch 2, global step 11000, total step 721440, train lm loss: 0.46988161973422393
epoch 2, global step 11010, total step 721440, train lm loss: 0.5047444082330912
epoch 2, global step 11020, total step 721440, train lm loss: 0.44702682782663034
epoch 2, global step 11030, total step 721440, train lm loss: 0.46543240200844593
epoch 2, global step 11040, total step 721440, train lm loss: 0.44355350934201854
epoch 2, global step 11050, total step 721440, train lm loss: 0.46140236302744597
epoch 2, global step 11060, total step 721440, train lm loss: 0.4663809436373413
epoch 2, global step 11070, total step 721440, train lm loss: 0.4856559101724997
epoch 2, global step 11080, total step 721440, train lm loss: 0.43511728858575227
epoch 2, global step 11090, total step 721440, train lm loss: 0.437190456665121
epoch 2, global step 11100, total step 721440, train lm loss: 0.4492173122358508
epoch 2, global step 11110, total step 721440, train lm loss: 0.4213626938522793
epoch 2, global step 11120, total step 721440, train lm loss: 0.448462962184567
epoch 2, global step 11130, total step 721440, train lm loss: 0.485838355217129
epoch 2, global step 11140, total step 721440, train lm loss: 0.45610952015267686
epoch 2, global step 11150, total step 721440, train lm loss: 0.4591692181653343
epoch 2, global step 11160, total step 721440, train lm loss: 0.4392696839524433
epoch 2, global step 11170, total step 721440, train lm loss: 0.432444929209305
epoch 2, global step 11180, total step 721440, train lm loss: 0.4540127220097929
epoch 2, global step 11190, total step 721440, train lm loss: 0.4626198149402626
epoch 2, global step 11200, total step 721440, train lm loss: 0.4923112168093212
epoch 2, global step 11210, total step 721440, train lm loss: 0.4523649026290514
epoch 2, global step 11220, total step 721440, train lm loss: 0.41712912439834327
epoch 2, global step 11230, total step 721440, train lm loss: 0.4747980852611363
epoch 2, global step 11240, total step 721440, train lm loss: 0.4037667202297598
epoch 2, global step 11250, total step 721440, train lm loss: 0.4359167205169797
epoch 2, global step 11260, total step 721440, train lm loss: 0.4370335120591335
epoch 2, global step 11270, total step 721440, train lm loss: 0.5067489496024791
epoch 2, global step 11280, total step 721440, train lm loss: 0.40518551438581196
epoch 2, global step 11290, total step 721440, train lm loss: 0.46403768594609573
epoch 2, global step 11300, total step 721440, train lm loss: 0.4656901796814054
epoch 2, global step 11310, total step 721440, train lm loss: 0.44488873758818953
epoch 2, global step 11320, total step 721440, train lm loss: 0.41301796967163684
epoch 2, global step 11330, total step 721440, train lm loss: 0.4193460573325865
epoch 2, global step 11340, total step 721440, train lm loss: 0.43015852260869
epoch 2, global step 11350, total step 721440, train lm loss: 0.4233501677168533
epoch 2, global step 11360, total step 721440, train lm loss: 0.4598779320251197
epoch 2, global step 11370, total step 721440, train lm loss: 0.4265188962337561
epoch 2, global step 11380, total step 721440, train lm loss: 0.4976591310463846
epoch 2, global step 11390, total step 721440, train lm loss: 0.43589091510511935
epoch 2, global step 11400, total step 721440, train lm loss: 0.4491776244714856
epoch 2, global step 11410, total step 721440, train lm loss: 0.40082796959904954
epoch 2, global step 11420, total step 721440, train lm loss: 0.4396367111010477
epoch 2, global step 11430, total step 721440, train lm loss: 0.44390677666524425
epoch 2, global step 11440, total step 721440, train lm loss: 0.42391026348341254
epoch 2, global step 11450, total step 721440, train lm loss: 0.4853584364405833
epoch 2, global step 11460, total step 721440, train lm loss: 0.47225284373853355
epoch 2, global step 11470, total step 721440, train lm loss: 0.3795127961435355
epoch 2, global step 11480, total step 721440, train lm loss: 0.3777209718246013
epoch 2, global step 11490, total step 721440, train lm loss: 0.40378981184912843
epoch 2, global step 11500, total step 721440, train lm loss: 0.36776145136682314
epoch 2, global step 11510, total step 721440, train lm loss: 0.4268397972220555
epoch 2, global step 11520, total step 721440, train lm loss: 0.47092964095645584
epoch 2, global step 11530, total step 721440, train lm loss: 0.4507931582746096
epoch 2, global step 11540, total step 721440, train lm loss: 0.45943156206049024
epoch 2, global step 11550, total step 721440, train lm loss: 0.4396288321586326
epoch 2, global step 11560, total step 721440, train lm loss: 0.4302612224710174
epoch 2, global step 11570, total step 721440, train lm loss: 0.4437623283243738
epoch 2, global step 11580, total step 721440, train lm loss: 0.42732742459047585
epoch 2, global step 11590, total step 721440, train lm loss: 0.4439053215784952
epoch 2, global step 11600, total step 721440, train lm loss: 0.44191690840525555
epoch 2, global step 11610, total step 721440, train lm loss: 0.4504127791617066
epoch 2, global step 11620, total step 721440, train lm loss: 0.45777651738608255
epoch 2, global step 11630, total step 721440, train lm loss: 0.4809879837092012
epoch 2, global step 11640, total step 721440, train lm loss: 0.4293880170909688
epoch 2, global step 11650, total step 721440, train lm loss: 0.3950039889663458
epoch 2, global step 11660, total step 721440, train lm loss: 0.4255273220827803
epoch 2, global step 11670, total step 721440, train lm loss: 0.41797559685073793
epoch 2, global step 11680, total step 721440, train lm loss: 0.43025365616194905
epoch 2, global step 11690, total step 721440, train lm loss: 0.4599518367787823
epoch 2, global step 11700, total step 721440, train lm loss: 0.4265975622693077
epoch 2, global step 11710, total step 721440, train lm loss: 0.40125318206846716
epoch 2, global step 11720, total step 721440, train lm loss: 0.45899160550907253
epoch 2, global step 11730, total step 721440, train lm loss: 0.43742134792264553
epoch 2, global step 11740, total step 721440, train lm loss: 0.4180809336248785
epoch 2, global step 11750, total step 721440, train lm loss: 0.4352262961445376
epoch 2, global step 11760, total step 721440, train lm loss: 0.4187239875027444
epoch 2, global step 11770, total step 721440, train lm loss: 0.44636843088082967
epoch 2, global step 11780, total step 721440, train lm loss: 0.3974528885562904
epoch 2, global step 11790, total step 721440, train lm loss: 0.41042968098190613
epoch 2, global step 11800, total step 721440, train lm loss: 0.45845105096232147
epoch 2, global step 11810, total step 721440, train lm loss: 0.4334444171952782
epoch 2, global step 11820, total step 721440, train lm loss: 0.38384766650851815
epoch 2, global step 11830, total step 721440, train lm loss: 0.4253779763472266
epoch 2, global step 11840, total step 721440, train lm loss: 0.4268166689260397
epoch 2, global step 11850, total step 721440, train lm loss: 0.45623048023553564
epoch 2, global step 11860, total step 721440, train lm loss: 0.42082852355670186
epoch 2, global step 11870, total step 721440, train lm loss: 0.4513945424812846
epoch 2, global step 11880, total step 721440, train lm loss: 0.3818414722220041
epoch 2, global step 11890, total step 721440, train lm loss: 0.3939024071092717
epoch 2, global step 11900, total step 721440, train lm loss: 0.399548339471221
epoch 2, global step 11910, total step 721440, train lm loss: 0.4267013282951666
epoch 2, global step 11920, total step 721440, train lm loss: 0.3771560441295151
epoch 2, global step 11930, total step 721440, train lm loss: 0.3592475773009937
epoch 2, global step 11940, total step 721440, train lm loss: 0.47619938233401626
epoch 2, global step 11950, total step 721440, train lm loss: 0.37817352723213843
epoch 2, global step 11960, total step 721440, train lm loss: 0.42021562011213975
epoch 2, global step 11970, total step 721440, train lm loss: 0.43737257953907827
epoch 2, global step 11980, total step 721440, train lm loss: 0.40111728967749516
epoch 2, global step 11990, total step 721440, train lm loss: 0.481136031053029
epoch 2, global step 12000, total step 721440, train lm loss: 0.4319181279512122
epoch 2, global step 12010, total step 721440, train lm loss: 0.40665608494891786
epoch 2, global step 12020, total step 721440, train lm loss: 0.4218754313304089
epoch 2, global step 12030, total step 721440, train lm loss: 0.4160364471958019
epoch 2, global step 12040, total step 721440, train lm loss: 0.37286823673639446
epoch 2, global step 12050, total step 721440, train lm loss: 0.4197125290287659
epoch 2, global step 12060, total step 721440, train lm loss: 0.39339057423640045
epoch 2, global step 12070, total step 721440, train lm loss: 0.43923376481980086
epoch 2, global step 12080, total step 721440, train lm loss: 0.40067155059659854
epoch 2, global step 12090, total step 721440, train lm loss: 0.3953090351773426
epoch 2, global step 12100, total step 721440, train lm loss: 0.4447055484983139
epoch 2, global step 12110, total step 721440, train lm loss: 0.4240851809736341
epoch 2, global step 12120, total step 721440, train lm loss: 0.40187150628771634
epoch 2, global step 12130, total step 721440, train lm loss: 0.45366142509737983
epoch 2, global step 12140, total step 721440, train lm loss: 0.39432612801901995
epoch 2, global step 12150, total step 721440, train lm loss: 0.41203325546812264
epoch 2, global step 12160, total step 721440, train lm loss: 0.4171703630476259
epoch 2, global step 12170, total step 721440, train lm loss: 0.3748793481674511
epoch 2, global step 12180, total step 721440, train lm loss: 0.3600216487655416
epoch 2, global step 12190, total step 721440, train lm loss: 0.39248696312424725
epoch 2, global step 12200, total step 721440, train lm loss: 0.3660880143637769
epoch 2, global step 12210, total step 721440, train lm loss: 0.4176339605357498
epoch 2, global step 12220, total step 721440, train lm loss: 0.4183747857692651
epoch 2, global step 12230, total step 721440, train lm loss: 0.4125355111202225
epoch 2, global step 12240, total step 721440, train lm loss: 0.41310815670294687
epoch 2, global step 12250, total step 721440, train lm loss: 0.3767880757921375
epoch 2, global step 12260, total step 721440, train lm loss: 0.3672380559146404
epoch 2, global step 12270, total step 721440, train lm loss: 0.3447573211742565
epoch 2, global step 12280, total step 721440, train lm loss: 0.41800715908175334
epoch 2, global step 12290, total step 721440, train lm loss: 0.3729958805255592
epoch 2, global step 12300, total step 721440, train lm loss: 0.37801416729344056
epoch 2, global step 12310, total step 721440, train lm loss: 0.39472808144055305
epoch 2, global step 12320, total step 721440, train lm loss: 0.3652263442345429
epoch 2, global step 12330, total step 721440, train lm loss: 0.3595440741017228
epoch 2, global step 12340, total step 721440, train lm loss: 0.34967087163822724
epoch 2, global step 12350, total step 721440, train lm loss: 0.4774613000219688
epoch 2, global step 12360, total step 721440, train lm loss: 0.41079996316693723
epoch 2, global step 12370, total step 721440, train lm loss: 0.3951236468506977
epoch 2, global step 12380, total step 721440, train lm loss: 0.39219930966501126
epoch 2, global step 12390, total step 721440, train lm loss: 0.489989873918239
epoch 2, global step 12400, total step 721440, train lm loss: 0.417811705643544
epoch 2, global step 12410, total step 721440, train lm loss: 0.4219820552971214
epoch 2, global step 12420, total step 721440, train lm loss: 0.3937783192901406
epoch 2, global step 12430, total step 721440, train lm loss: 0.3909060274367221
epoch 2, global step 12440, total step 721440, train lm loss: 0.41030306940083394
epoch 2, global step 12450, total step 721440, train lm loss: 0.40424770007375627
epoch 2, global step 12460, total step 721440, train lm loss: 0.3760071741882712
epoch 2, global step 12470, total step 721440, train lm loss: 0.4005761459993664
epoch 2, global step 12480, total step 721440, train lm loss: 0.37855243726517074
epoch 2, global step 12490, total step 721440, train lm loss: 0.37613701406808103
epoch 2, global step 12500, total step 721440, train lm loss: 0.39120542726013807
epoch 2, global step 12510, total step 721440, train lm loss: 0.3517507397395093
epoch 2, global step 12520, total step 721440, train lm loss: 0.4251809550565667
epoch 2, global step 12530, total step 721440, train lm loss: 0.40196486867498604
epoch 2, global step 12540, total step 721440, train lm loss: 0.37617146923439576
epoch 2, global step 12550, total step 721440, train lm loss: 0.36244497521547603
epoch 2, global step 12560, total step 721440, train lm loss: 0.32512753835762853
epoch 2, global step 12570, total step 721440, train lm loss: 0.37448678727960216
epoch 2, global step 12580, total step 721440, train lm loss: 0.40050519243814053
epoch 2, global step 12590, total step 721440, train lm loss: 0.4309990939567797
epoch 2, global step 12600, total step 721440, train lm loss: 0.4327087634708732
epoch 2, global step 12610, total step 721440, train lm loss: 0.37682524804840795
epoch 2, global step 12620, total step 721440, train lm loss: 0.3848592865804676
epoch 2, global step 12630, total step 721440, train lm loss: 0.3454631670494564
epoch 2, global step 12640, total step 721440, train lm loss: 0.3850671760039404
epoch 2, global step 12650, total step 721440, train lm loss: 0.43021710818866266
epoch 2, global step 12660, total step 721440, train lm loss: 0.3720273357932456
epoch 2, global step 12670, total step 721440, train lm loss: 0.4110078966768924
epoch 2, global step 12680, total step 721440, train lm loss: 0.392508879583329
epoch 2, global step 12690, total step 721440, train lm loss: 0.3408268261177
epoch 2, global step 12700, total step 721440, train lm loss: 0.3452823401254136
epoch 2, global step 12710, total step 721440, train lm loss: 0.38421133122174067
epoch 2, global step 12720, total step 721440, train lm loss: 0.3654518262075726
epoch 2, global step 12730, total step 721440, train lm loss: 0.39456483339890835
epoch 2, global step 12740, total step 721440, train lm loss: 0.3411473121901508
epoch 2, global step 12750, total step 721440, train lm loss: 0.42128110899939203
epoch 2, global step 12760, total step 721440, train lm loss: 0.3345490026345942
epoch 2, global step 12770, total step 721440, train lm loss: 0.37149668562924487
epoch 2, global step 12780, total step 721440, train lm loss: 0.3792725501931272
epoch 2, global step 12790, total step 721440, train lm loss: 0.43036166167585177
epoch 2, global step 12800, total step 721440, train lm loss: 0.3441581114951987
epoch 2, global step 12810, total step 721440, train lm loss: 0.3597057291481178
epoch 2, global step 12820, total step 721440, train lm loss: 0.3878127895994112
epoch 2, global step 12830, total step 721440, train lm loss: 0.3906420148559846
epoch 2, global step 12840, total step 721440, train lm loss: 0.3416337997070514
epoch 2, global step 12850, total step 721440, train lm loss: 0.36620519940042867
epoch 2, global step 12860, total step 721440, train lm loss: 0.3706166180665605
epoch 2, global step 12870, total step 721440, train lm loss: 0.3684173893299885
epoch 2, global step 12880, total step 721440, train lm loss: 0.4040654904005351
epoch 2, global step 12890, total step 721440, train lm loss: 0.3745101628941484
epoch 2, global step 12900, total step 721440, train lm loss: 0.3781177346478216
epoch 2, global step 12910, total step 721440, train lm loss: 0.38783816987415776
epoch 2, global step 12920, total step 721440, train lm loss: 0.35198608189821246
epoch 2, global step 12930, total step 721440, train lm loss: 0.3365999370464124
epoch 2, global step 12940, total step 721440, train lm loss: 0.33487285663140937
epoch 2, global step 12950, total step 721440, train lm loss: 0.32549938646843657
epoch 2, global step 12960, total step 721440, train lm loss: 0.39689527862938123
epoch 2, global step 12970, total step 721440, train lm loss: 0.37247675356338733
epoch 2, global step 12980, total step 721440, train lm loss: 0.3413633109070361
epoch 2, global step 12990, total step 721440, train lm loss: 0.3719056379515678
epoch 2, global step 13000, total step 721440, train lm loss: 0.3419919270440005
epoch 2, global step 13010, total step 721440, train lm loss: 0.4080150961817708
epoch 2, global step 13020, total step 721440, train lm loss: 0.36615129276760855
epoch 2, global step 13030, total step 721440, train lm loss: 0.3545931184082292
epoch 2, global step 13040, total step 721440, train lm loss: 0.3717994412756525
epoch 2, global step 13050, total step 721440, train lm loss: 0.3599082597764209
epoch 2, global step 13060, total step 721440, train lm loss: 0.3438109634560533
epoch 2, global step 13070, total step 721440, train lm loss: 0.37358430547756144
epoch 2, global step 13080, total step 721440, train lm loss: 0.3703109862923156
epoch 2, global step 13090, total step 721440, train lm loss: 0.36766145561123265
epoch 2, global step 13100, total step 721440, train lm loss: 0.35746307934168725
epoch 2, global step 13110, total step 721440, train lm loss: 0.31657475136744323
epoch 2, global step 13120, total step 721440, train lm loss: 0.3418641995987855
epoch 2, global step 13130, total step 721440, train lm loss: 0.3469561700243503
epoch 2, global step 13140, total step 721440, train lm loss: 0.36182486242614686
epoch 2, global step 13150, total step 721440, train lm loss: 0.3193927810789319
epoch 2, global step 13160, total step 721440, train lm loss: 0.3628123259055428
epoch 2, global step 13170, total step 721440, train lm loss: 0.3998999367293436
epoch 2, global step 13180, total step 721440, train lm loss: 0.3484647345874691
epoch 2, global step 13190, total step 721440, train lm loss: 0.38737177634611725
epoch 2, global step 13200, total step 721440, train lm loss: 0.3831449216231704
epoch 2, global step 13210, total step 721440, train lm loss: 0.3472908988886047
epoch 2, global step 13220, total step 721440, train lm loss: 0.3809528379235417
epoch 2, global step 13230, total step 721440, train lm loss: 0.346232272265479
epoch 2, global step 13240, total step 721440, train lm loss: 0.3593032023403794
epoch 2, global step 13250, total step 721440, train lm loss: 0.33996046901447696
epoch 2, global step 13260, total step 721440, train lm loss: 0.34575355251436124
epoch 2, global step 13270, total step 721440, train lm loss: 0.3751158084080089
epoch 2, global step 13280, total step 721440, train lm loss: 0.3769881295855157
epoch 2, global step 13290, total step 721440, train lm loss: 0.3326122455007862
epoch 2, global step 13300, total step 721440, train lm loss: 0.35521303268324117
epoch 2, global step 13310, total step 721440, train lm loss: 0.3452147313626483
epoch 2, global step 13320, total step 721440, train lm loss: 0.3536788280121982
epoch 2, global step 13330, total step 721440, train lm loss: 0.383440219599288
epoch 2, global step 13340, total step 721440, train lm loss: 0.3663185427139979
epoch 2, global step 13350, total step 721440, train lm loss: 0.2781552781671053
epoch 2, global step 13360, total step 721440, train lm loss: 0.350440972862998
epoch 2, global step 13370, total step 721440, train lm loss: 0.38276262565050273
epoch 2, global step 13380, total step 721440, train lm loss: 0.3198000139091164
